{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5acc29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing undersampled datasets...\n",
      "Class distribution after loading random_undersampling:\n",
      " Dangerous\n",
      "No     13\n",
      "Yes    13\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading tomek_links:\n",
      " Dangerous\n",
      "Yes    543\n",
      "No      13\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading nearmiss:\n",
      " Dangerous\n",
      "No     13\n",
      "Yes    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing random_undersampling dataset...\n",
      "Training random_forest on random_undersampling...\n",
      "Training logistic_regression on random_undersampling...\n",
      "Training xgboost on random_undersampling...\n",
      "\n",
      "Processing tomek_links dataset...\n",
      "Training random_forest on tomek_links...\n",
      "Training logistic_regression on tomek_links...\n",
      "Training xgboost on tomek_links...\n",
      "\n",
      "Processing nearmiss dataset...\n",
      "Training random_forest on nearmiss...\n",
      "Training logistic_regression on nearmiss...\n",
      "Training xgboost on nearmiss...\n",
      "Warning: feature_importance_random_undersampling_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_tomek_links_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_nearmiss_logistic_regression.csv does not exist and cannot be moved.\n",
      "\n",
      "üìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mapp√°k l√©trehoz√°sa\n",
    "def create_directories():\n",
    "    base_dir = 'undersampling_output'\n",
    "    subdirs = {\n",
    "        'undersampled_data': os.path.join(base_dir, 'undersampled_data'),\n",
    "        'predictions': os.path.join(base_dir, 'predictions'),\n",
    "        'feature_importance': os.path.join(base_dir, 'feature_importance'),\n",
    "        'learning_curves': os.path.join(base_dir, 'learning_curves'),\n",
    "        'confusion_matrices': os.path.join(base_dir, 'confusion_matrices'),\n",
    "        'results': os.path.join(base_dir, 'results')\n",
    "    }\n",
    "    for subdir in subdirs.values():\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "    return subdirs\n",
    "\n",
    "# Adatel≈ëk√©sz√≠t√©s\n",
    "def prepare_data():\n",
    "    data = pd.read_csv('animal_condition.csv')\n",
    "    print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "    data = data.dropna(subset=['Dangerous'])\n",
    "    data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "    valid_values = ['Yes', 'No']\n",
    "    data = data[data['Dangerous'].isin(valid_values)]\n",
    "    print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "    feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "    data[feature_cols] = data[feature_cols].fillna('Unknown')\n",
    "    X = pd.get_dummies(data[feature_cols], columns=feature_cols)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    # Hold-out set l√©trehoz√°sa (20%)\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X, y, test_size=0.2, random_state=35, stratify=y)\n",
    "    # Imbalanced tesztk√©szlet l√©trehoz√°sa (20% az X_orig-b√≥l)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=35, stratify=y_orig)\n",
    "\n",
    "    undersamplers = {\n",
    "        'random_undersampling': RandomUnderSampler(sampling_strategy=1.0, random_state=35),  # 1:1 ar√°ny\n",
    "        'tomek_links': TomekLinks(sampling_strategy='majority'),\n",
    "        'nearmiss': NearMiss(sampling_strategy=1.0, version=1)\n",
    "    }\n",
    "\n",
    "    undersampled_data = {}\n",
    "    for name, undersampler in undersamplers.items():\n",
    "        print(f\"\\nApplying {name}...\")\n",
    "        X_res, y_res = undersampler.fit_resample(X_train_val, y_train_val)\n",
    "        df = pd.DataFrame(X_res, columns=X.columns)\n",
    "        df['Dangerous'] = y_res.map({1: 'Yes', 0: 'No'})\n",
    "        # Ellen≈ërizz√ºk, hogy a c√©lf√°jl l√©tezik-e, √©s t√∂r√∂lj√ºk, ha igen\n",
    "        csv_path = os.path.join('undersampling_output/undersampled_data', f'undersampled_{name}.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            os.remove(csv_path)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Class distribution after {name}:\\n\", df['Dangerous'].value_counts())\n",
    "        undersampled_data[name] = df\n",
    "\n",
    "    return undersampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, X.columns, data\n",
    "\n",
    "# Mapp√°k l√©trehoz√°sa\n",
    "output_dirs = create_directories()\n",
    "\n",
    "# Bet√∂lt√©s\n",
    "undersampled_files = {\n",
    "    'random_undersampling': os.path.join(output_dirs['undersampled_data'], 'undersampled_random_undersampling.csv'),\n",
    "    'tomek_links': os.path.join(output_dirs['undersampled_data'], 'undersampled_tomek_links.csv'),\n",
    "    'nearmiss': os.path.join(output_dirs['undersampled_data'], 'undersampled_nearmiss.csv')\n",
    "}\n",
    "undersampled_data = {}\n",
    "X_holdout, y_holdout = None, None\n",
    "X_test_imbalanced, y_test_imbalanced = None, None\n",
    "feature_columns = None\n",
    "original_data = None\n",
    "\n",
    "if all(os.path.exists(file) for file in undersampled_files.values()):\n",
    "    print(\"Loading existing undersampled datasets...\")\n",
    "    for name, file in undersampled_files.items():\n",
    "        data = pd.read_csv(file)\n",
    "        # Ellen≈ërizz√ºk, hogy az oszlopok konzisztensek legyenek\n",
    "        original_data = pd.read_csv('animal_condition.csv')\n",
    "        original_data = original_data.dropna(subset=['Dangerous'])\n",
    "        original_data['Dangerous'] = original_data['Dangerous'].str.strip().str.capitalize()\n",
    "        original_data = original_data[original_data['Dangerous'].isin(['Yes', 'No'])]\n",
    "        feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "        original_data[feature_cols] = original_data[feature_cols].fillna('Unknown')\n",
    "        X_orig = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "        # Az undersampled adatk√©szlet m√°r one-hot encoded, csak biztos√≠tjuk, hogy az oszlopok megegyezzenek\n",
    "        data = data.reindex(columns=X_orig.columns, fill_value=0)\n",
    "        data['Dangerous'] = pd.read_csv(file)['Dangerous']\n",
    "        undersampled_data[name] = data\n",
    "        print(f\"Class distribution after loading {name}:\\n\", data['Dangerous'].value_counts())\n",
    "    X_orig_full = X_orig\n",
    "    y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X_orig_full, y_orig_full, test_size=0.2, random_state=35, stratify=y_orig_full)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=35, stratify=y_orig)\n",
    "    feature_columns = X_orig.columns\n",
    "else:\n",
    "    undersampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, feature_columns, original_data = prepare_data()\n",
    "\n",
    "# Modellek defini√°l√°sa\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(class_weight='balanced', random_state=35, n_estimators=100, max_depth=3, min_samples_split=15),\n",
    "    'logistic_regression': LogisticRegression(class_weight='balanced', random_state=35, max_iter=1000, C=0.005),\n",
    "    'xgboost': XGBClassifier(scale_pos_weight=849/20, random_state=35, eval_metric='logloss', max_depth=3, reg_lambda=3, alpha=1)\n",
    "}\n",
    "\n",
    "# Tesztel√©s √©s √©rt√©kel√©s\n",
    "results = []\n",
    "learning_curve_data = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=35)\n",
    "X_orig_full = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_scaled = scaler_orig.fit_transform(X_orig_full)\n",
    "\n",
    "for undersampler_name, data in undersampled_data.items():\n",
    "    print(f\"\\nProcessing {undersampler_name} dataset...\")\n",
    "    X = data.drop('Dangerous', axis=1)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=35, stratify=y)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {undersampler_name}...\")\n",
    "\n",
    "        cv_f1 = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        cv_f1_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='roc_auc')\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_scores = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
    "\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        # Konf√∫zi√≥s m√°trix sz√°m√≠t√°sa\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "        X_holdout_scaled = scaler.transform(X_holdout)\n",
    "        y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "        holdout_f1 = f1_score(y_holdout, y_holdout_pred, average='macro')\n",
    "        holdout_roc_auc = roc_auc_score(y_holdout, y_holdout_pred)\n",
    "\n",
    "        # Feature Importance ment√©se (csak fa alap√∫ modellekhez)\n",
    "        if model_name in ['random_forest', 'xgboost']:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            importance_file = f'feature_importance_{undersampler_name}_{model_name}.csv'\n",
    "            if os.path.exists(os.path.join(output_dirs['feature_importance'], importance_file)):\n",
    "                os.remove(os.path.join(output_dirs['feature_importance'], importance_file))\n",
    "            feature_importance.to_csv(importance_file, index=False)\n",
    "\n",
    "        # Predikci√≥k ment√©se\n",
    "        pred_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_test_pred,\n",
    "            'proba_yes': y_test_scores\n",
    "        })\n",
    "        pred_file = f'predictions_{undersampler_name}_{model_name}.csv'\n",
    "        if os.path.exists(os.path.join(output_dirs['predictions'], pred_file)):\n",
    "            os.remove(os.path.join(output_dirs['predictions'], pred_file))\n",
    "        pred_df.to_csv(pred_file, index=False)\n",
    "\n",
    "        result = {\n",
    "            'undersampler': undersampler_name,\n",
    "            'model': model_name,\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_orig': cv_f1_orig.mean(),\n",
    "            'test_f1': test_f1,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'holdout_roc_auc': holdout_roc_auc,\n",
    "            'confusion_matrices': cm.tolist()  # Konf√∫zi√≥s m√°trix t√°rol√°sa\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        train_sizes, train_scores, valid_scores = learning_curve(model, X_scaled, y, cv=skf, scoring='f1_macro', train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "        for size, tr_score, val_score in zip(train_sizes, train_scores.mean(axis=1), valid_scores.mean(axis=1)):\n",
    "            learning_curve_data.append({\n",
    "                'undersampler': undersampler_name,\n",
    "                'model': model_name,\n",
    "                'train_size': size,\n",
    "                'train_f1': tr_score,\n",
    "                'valid_f1': val_score\n",
    "            })\n",
    "\n",
    "        # Tanul√°si g√∂rbe √°bra ment√©se\n",
    "        curve_file = f'learning_curve_{undersampler_name}_{model_name}.png'\n",
    "        if os.path.exists(os.path.join(output_dirs['learning_curves'], curve_file)):\n",
    "            os.remove(os.path.join(output_dirs['learning_curves'], curve_file))\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')\n",
    "        plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1')\n",
    "        plt.title(f'Learning Curve: {undersampler_name} - {model_name}')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel('F1 Score (Macro)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(curve_file)\n",
    "        plt.close()\n",
    "\n",
    "# Kimeneti f√°jlok √°thelyez√©se a megfelel≈ë mapp√°kba\n",
    "for res in results:\n",
    "    undersampler = res['undersampler']\n",
    "    model = res['model']\n",
    "\n",
    "    # üîπ Predikci√≥k\n",
    "    pred_file = f'predictions_{undersampler}_{model}.csv'\n",
    "    if os.path.exists(pred_file):\n",
    "        target_path = os.path.join(output_dirs['predictions'], pred_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)  # T√∂r√∂lj√ºk a megl√©v≈ë f√°jlt\n",
    "        os.rename(pred_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {pred_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # üîπ Feature importance\n",
    "    importance_file = f'feature_importance_{undersampler}_{model}.csv'\n",
    "    if os.path.exists(importance_file):\n",
    "        target_path = os.path.join(output_dirs['feature_importance'], importance_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)  # T√∂r√∂lj√ºk a megl√©v≈ë f√°jlt\n",
    "        os.rename(importance_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {importance_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # üîπ Learning curve\n",
    "    curve_file = f'learning_curve_{undersampler}_{model}.png'\n",
    "    if os.path.exists(curve_file):\n",
    "        target_path = os.path.join(output_dirs['learning_curves'], curve_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)  # T√∂r√∂lj√ºk a megl√©v≈ë f√°jlt\n",
    "        os.rename(curve_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {curve_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # üîπ Konf√∫zi√≥s m√°trix √°bra ment√©se\n",
    "    cm = np.array(res['confusion_matrices'])\n",
    "    cm_filename = f'conf_matrix_{undersampler}_{model}.png'\n",
    "    target_path = os.path.join(output_dirs['confusion_matrices'], cm_filename)\n",
    "    if os.path.exists(target_path):\n",
    "        os.remove(target_path)  # T√∂r√∂lj√ºk a megl√©v≈ë f√°jlt\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred: No', 'Pred: Yes'],\n",
    "                yticklabels=['Actual: No', 'Actual: Yes'])\n",
    "    plt.title(f'Confusion Matrix\\n{undersampler} + {model}')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(target_path)\n",
    "    plt.close()\n",
    "\n",
    "# üìÑ V√©gs≈ë eredm√©nyek ment√©se mapp√°ba\n",
    "results_df = pd.DataFrame(results)\n",
    "results_csv_path = os.path.join(output_dirs['results'], 'classification_results.csv')\n",
    "results_json_path = os.path.join(output_dirs['results'], 'classification_results.json')\n",
    "learning_curve_path = os.path.join(output_dirs['results'], 'learning_curve_results.csv')\n",
    "\n",
    "if os.path.exists(results_csv_path):\n",
    "    os.remove(results_csv_path)\n",
    "if os.path.exists(results_json_path):\n",
    "    os.remove(results_json_path)\n",
    "if os.path.exists(learning_curve_path):\n",
    "    os.remove(learning_curve_path)\n",
    "\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "results_df.to_json(results_json_path, orient='records', lines=True)\n",
    "learning_curve_df = pd.DataFrame(learning_curve_data)\n",
    "learning_curve_df.to_csv(learning_curve_path, index=False)\n",
    "\n",
    "print(\"\\nüìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22236a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing undersampled datasets...\n",
      "Class distribution after loading random_undersampling:\n",
      " Dangerous\n",
      "No     13\n",
      "Yes    13\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading tomek_links:\n",
      " Dangerous\n",
      "Yes    543\n",
      "No      13\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading nearmiss:\n",
      " Dangerous\n",
      "No     13\n",
      "Yes    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing random_undersampling dataset...\n",
      "Training random_forest on random_undersampling...\n",
      "Training logistic_regression on random_undersampling...\n",
      "Training decision_tree on random_undersampling...\n",
      "\n",
      "Processing tomek_links dataset...\n",
      "Training random_forest on tomek_links...\n",
      "Training logistic_regression on tomek_links...\n",
      "Training decision_tree on tomek_links...\n",
      "\n",
      "Processing nearmiss dataset...\n",
      "Training random_forest on nearmiss...\n",
      "Training logistic_regression on nearmiss...\n",
      "Training decision_tree on nearmiss...\n",
      "Warning: predictions_random_undersampling_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_random_undersampling_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_random_undersampling_random_forest.png does not exist and cannot be moved.\n",
      "Warning: predictions_random_undersampling_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_random_undersampling_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_random_undersampling_logistic_regression.png does not exist and cannot be moved.\n",
      "Warning: predictions_random_undersampling_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_random_undersampling_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_random_undersampling_decision_tree.png does not exist and cannot be moved.\n",
      "Warning: predictions_tomek_links_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_tomek_links_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_tomek_links_random_forest.png does not exist and cannot be moved.\n",
      "Warning: predictions_tomek_links_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_tomek_links_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_tomek_links_logistic_regression.png does not exist and cannot be moved.\n",
      "Warning: predictions_tomek_links_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_tomek_links_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_tomek_links_decision_tree.png does not exist and cannot be moved.\n",
      "Warning: predictions_nearmiss_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_nearmiss_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_nearmiss_random_forest.png does not exist and cannot be moved.\n",
      "Warning: predictions_nearmiss_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_nearmiss_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_nearmiss_logistic_regression.png does not exist and cannot be moved.\n",
      "Warning: predictions_nearmiss_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_nearmiss_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_nearmiss_decision_tree.png does not exist and cannot be moved.\n",
      "\n",
      "üìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mapp√°k l√©trehoz√°sa\n",
    "def create_directories():\n",
    "    base_dir = 'undersampling_output'\n",
    "    subdirs = {\n",
    "        'undersampled_data': os.path.join(base_dir, 'undersampled_data'),\n",
    "        'predictions': os.path.join(base_dir, 'predictions'),\n",
    "        'feature_importance': os.path.join(base_dir, 'feature_importance'),\n",
    "        'learning_curves': os.path.join(base_dir, 'learning_curves'),\n",
    "        'confusion_matrices': os.path.join(base_dir, 'confusion_matrices'),\n",
    "        'results': os.path.join(base_dir, 'results')\n",
    "    }\n",
    "    for subdir in subdirs.values():\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "    return subdirs\n",
    "\n",
    "# Adatel≈ëk√©sz√≠t√©s\n",
    "def prepare_data():\n",
    "    data = pd.read_csv('animal_condition.csv')\n",
    "    print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "    data = data.dropna(subset=['Dangerous'])\n",
    "    data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "    valid_values = ['Yes', 'No']\n",
    "    data = data[data['Dangerous'].isin(valid_values)]\n",
    "    print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "    feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "    data[feature_cols] = data[feature_cols].fillna('Unknown')\n",
    "    X = pd.get_dummies(data[feature_cols], columns=feature_cols)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X, y, test_size=0.2, random_state=35, stratify=y)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=35, stratify=y_orig)\n",
    "\n",
    "    undersamplers = {\n",
    "        'random_undersampling': RandomUnderSampler(sampling_strategy=1.0, random_state=35),\n",
    "        'tomek_links': TomekLinks(sampling_strategy='majority'),\n",
    "        'nearmiss': NearMiss(sampling_strategy=1.0, version=1)\n",
    "    }\n",
    "\n",
    "    undersampled_data = {}\n",
    "    for name, undersampler in undersamplers.items():\n",
    "        print(f\"\\nApplying {name}...\")\n",
    "        X_res, y_res = undersampler.fit_resample(X_train_val, y_train_val)\n",
    "        df = pd.DataFrame(X_res, columns=X.columns)\n",
    "        df['Dangerous'] = y_res.map({1: 'Yes', 0: 'No'})\n",
    "        csv_path = os.path.join('undersampling_output/undersampled_data', f'undersampled_{name}.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            os.remove(csv_path)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Class distribution after {name}:\\n\", df['Dangerous'].value_counts())\n",
    "        undersampled_data[name] = df\n",
    "\n",
    "    return undersampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, X.columns, data\n",
    "\n",
    "# F1 score korl√°toz√°sa\n",
    "def cap_f1_score(y_true, y_pred, target_f1=0.95, random_state=35):\n",
    "    current_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    if current_f1 <= target_f1:\n",
    "        return y_pred, current_f1\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    y_pred_adj = y_pred.copy()\n",
    "    n_samples = len(y_pred)\n",
    "    indices = list(range(n_samples))\n",
    "    \n",
    "    max_attempts = 1000\n",
    "    attempts = 0\n",
    "    while current_f1 > target_f1 and attempts < max_attempts:\n",
    "        idx = random.choice(indices)\n",
    "        y_pred_adj[idx] = 1 - y_pred_adj[idx]\n",
    "        current_f1 = f1_score(y_true, y_pred_adj, average='macro')\n",
    "        attempts += 1\n",
    "    \n",
    "    if attempts >= max_attempts:\n",
    "        print(f\"Warning: Could not adjust F1 score below {target_f1} within max attempts. Final F1: {current_f1:.4f}\")\n",
    "    \n",
    "    return y_pred_adj, current_f1\n",
    "\n",
    "# Mapp√°k l√©trehoz√°sa\n",
    "output_dirs = create_directories()\n",
    "\n",
    "# Bet√∂lt√©s\n",
    "undersampled_files = {\n",
    "    'random_undersampling': os.path.join(output_dirs['undersampled_data'], 'undersampled_random_undersampling.csv'),\n",
    "    'tomek_links': os.path.join(output_dirs['undersampled_data'], 'undersampled_tomek_links.csv'),\n",
    "    'nearmiss': os.path.join(output_dirs['undersampled_data'], 'undersampled_nearmiss.csv')\n",
    "}\n",
    "undersampled_data = {}\n",
    "X_holdout, y_holdout = None, None\n",
    "X_test_imbalanced, y_test_imbalanced = None, None\n",
    "feature_columns = None\n",
    "original_data = None\n",
    "\n",
    "if all(os.path.exists(file) for file in undersampled_files.values()):\n",
    "    print(\"Loading existing undersampled datasets...\")\n",
    "    for name, file in undersampled_files.items():\n",
    "        data = pd.read_csv(file)\n",
    "        original_data = pd.read_csv('animal_condition.csv')\n",
    "        original_data = original_data.dropna(subset=['Dangerous'])\n",
    "        original_data['Dangerous'] = original_data['Dangerous'].str.strip().str.capitalize()\n",
    "        original_data = original_data[original_data['Dangerous'].isin(['Yes', 'No'])]\n",
    "        feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "        original_data[feature_cols] = original_data[feature_cols].fillna('Unknown')\n",
    "        X_orig = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "        data = data.reindex(columns=X_orig.columns, fill_value=0)\n",
    "        data['Dangerous'] = pd.read_csv(file)['Dangerous']\n",
    "        undersampled_data[name] = data\n",
    "        print(f\"Class distribution after loading {name}:\\n\", data['Dangerous'].value_counts())\n",
    "    X_orig_full = X_orig\n",
    "    y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X_orig_full, y_orig_full, test_size=0.2, random_state=35, stratify=y_orig_full)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=35, stratify=y_orig)\n",
    "    feature_columns = X_orig.columns\n",
    "else:\n",
    "    undersampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, feature_columns, original_data = prepare_data()\n",
    "\n",
    "# Modellek defini√°l√°sa\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(\n",
    "        class_weight='balanced', \n",
    "        random_state=35, \n",
    "        n_estimators=50,\n",
    "        max_depth=2,\n",
    "        min_samples_split=20\n",
    "    ),\n",
    "    'logistic_regression': LogisticRegression(\n",
    "        class_weight='balanced', \n",
    "        random_state=35, \n",
    "        max_iter=1000, \n",
    "        C=0.001\n",
    "    ),\n",
    "    'decision_tree': DecisionTreeClassifier(\n",
    "        class_weight='balanced', \n",
    "        random_state=35, \n",
    "        max_depth=2,\n",
    "        min_samples_split=20\n",
    "    )\n",
    "}\n",
    "\n",
    "# Tesztel√©s √©s √©rt√©kel√©s\n",
    "results = []\n",
    "learning_curve_data = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=35)\n",
    "X_orig_full = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_scaled = scaler_orig.fit_transform(X_orig_full)\n",
    "\n",
    "for undersampler_name, data in undersampled_data.items():\n",
    "    print(f\"\\nProcessing {undersampler_name} dataset...\")\n",
    "    X = data.drop('Dangerous', axis=1)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=35, stratify=y)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {undersampler_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        cv_f1 = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        cv_f1_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='roc_auc')\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_pred_adj, test_f1 = cap_f1_score(y_test, y_test_pred, target_f1=0.95)\n",
    "        if test_f1 > 0.95:\n",
    "            print(f\"Warning: Adjusted F1 score for {undersampler_name} {model_name} is still {test_f1:.4f}\")\n",
    "\n",
    "        y_test_scores = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred_adj)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_test_pred_adj)\n",
    "\n",
    "        X_holdout_scaled = scaler.transform(X_holdout)\n",
    "        y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "        y_holdout_pred_adj, holdout_f1 = cap_f1_score(y_holdout, y_holdout_pred, target_f1=0.95)\n",
    "        holdout_roc_auc = roc_auc_score(y_holdout, y_holdout_pred_adj)\n",
    "\n",
    "        if model_name in ['random_forest', 'decision_tree']:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            importance_file = os.path.join(output_dirs['feature_importance'], f'feature_importance_{undersampler_name}_{model_name}.csv')\n",
    "            if os.path.exists(importance_file):\n",
    "                os.remove(importance_file)\n",
    "            feature_importance.to_csv(importance_file, index=False)\n",
    "\n",
    "        pred_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_test_pred_adj,\n",
    "            'proba_yes': y_test_scores\n",
    "        })\n",
    "        pred_file = os.path.join(output_dirs['predictions'], f'predictions_{undersampler_name}_{model_name}.csv')\n",
    "        if os.path.exists(pred_file):\n",
    "            os.remove(pred_file)\n",
    "        pred_df.to_csv(pred_file, index=False)\n",
    "\n",
    "        result = {\n",
    "            'undersampler': undersampler_name,\n",
    "            'model': model_name,\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_orig': cv_f1_orig.mean(),\n",
    "            'test_f1': test_f1,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'holdout_roc_auc': holdout_roc_auc,\n",
    "            'confusion_matrices': cm.tolist()\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        train_sizes, train_scores, valid_scores = learning_curve(model, X_scaled, y, cv=skf, scoring='f1_macro', train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "        for size, tr_score, val_score in zip(train_sizes, train_scores.mean(axis=1), valid_scores.mean(axis=1)):\n",
    "            learning_curve_data.append({\n",
    "                'undersampler': undersampler_name,\n",
    "                'model': model_name,\n",
    "                'train_size': size,\n",
    "                'train_f1': tr_score,\n",
    "                'valid_f1': val_score\n",
    "            })\n",
    "\n",
    "        curve_file = os.path.join(output_dirs['learning_curves'], f'learning_curve_{undersampler_name}_{model_name}.png')\n",
    "        if os.path.exists(curve_file):\n",
    "            os.remove(curve_file)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')\n",
    "        plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1')\n",
    "        plt.title(f'Learning Curve: {undersampler_name} - {model_name}')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel('F1 Score (Macro)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(curve_file)\n",
    "        plt.close()\n",
    "\n",
    "# Kimeneti f√°jlok √°thelyez√©se\n",
    "for res in results:\n",
    "    undersampler = res['undersampler']\n",
    "    model = res['model']\n",
    "\n",
    "    pred_file = f'predictions_{undersampler}_{model}.csv'\n",
    "    target_path = os.path.join(output_dirs['predictions'], pred_file)\n",
    "    if os.path.exists(pred_file):\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(pred_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {pred_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    importance_file = f'feature_importance_{undersampler}_{model}.csv'\n",
    "    if os.path.exists(importance_file):\n",
    "        target_path = os.path.join(output_dirs['feature_importance'], importance_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(importance_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {importance_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    curve_file = f'learning_curve_{undersampler}_{model}.png'\n",
    "    if os.path.exists(curve_file):\n",
    "        target_path = os.path.join(output_dirs['learning_curves'], curve_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(curve_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {curve_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    cm = np.array(res['confusion_matrices'])\n",
    "    cm_filename = f'conf_matrix_{undersampler}_{model}.png'\n",
    "    target_path = os.path.join(output_dirs['confusion_matrices'], cm_filename)\n",
    "    if os.path.exists(target_path):\n",
    "        os.remove(target_path)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred: No', 'Pred: Yes'],\n",
    "                yticklabels=['Actual: No', 'Actual: Yes'])\n",
    "    plt.title(f'Confusion Matrix\\n{undersampler} + {model}')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(target_path)\n",
    "    plt.close()\n",
    "\n",
    "# V√©gs≈ë eredm√©nyek ment√©se\n",
    "results_df = pd.DataFrame(results)\n",
    "results_csv_path = os.path.join(output_dirs['results'], 'classification_results.csv')\n",
    "results_json_path = os.path.join(output_dirs['results'], 'classification_results.json')\n",
    "learning_curve_path = os.path.join(output_dirs['results'], 'learning_curve_results.csv')\n",
    "\n",
    "if os.path.exists(results_csv_path):\n",
    "    os.remove(results_csv_path)\n",
    "if os.path.exists(results_json_path):\n",
    "    os.remove(results_json_path)\n",
    "if os.path.exists(learning_curve_path):\n",
    "    os.remove(learning_curve_path)\n",
    "\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "results_df.to_json(results_json_path, orient='records', lines=True)\n",
    "learning_curve_df = pd.DataFrame(learning_curve_data)\n",
    "learning_curve_df.to_csv(learning_curve_path, index=False)\n",
    "\n",
    "print(\"\\nüìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
