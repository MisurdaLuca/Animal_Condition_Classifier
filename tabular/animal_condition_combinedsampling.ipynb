{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f675cf1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\__init__.py:84\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isfinite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[0;32m     13\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_api_compat.numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\fixes.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\__init__.py:610\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    609\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 610\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, _get_nan,\n\u001b[0;32m     42\u001b[0m                               _rename_parameter, _contains_nan,\n\u001b[0;32m     43\u001b[0m                               AxisError)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\__init__.py:427\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_nnls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nnls\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_basinhopping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m basinhopping\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linprog, linprog_verbose_callback\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lsap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_differentialevolution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m differential_evolution\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_linprog.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizeResult, OptimizeWarning\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog_highs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _linprog_highs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog_ip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _linprog_ip\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linprog_simplex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _linprog_simplex\n",
      "File \u001b[1;32mc:\\Users\\misur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_linprog_highs.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizeWarning, OptimizeResult\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_highs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_highs_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _highs_wrapper\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_highs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_highs_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     CONST_INF,\n\u001b[0;32m     23\u001b[0m     MESSAGE_LEVEL_NONE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     HIGHS_SIMPLEX_EDGE_WEIGHT_STRATEGY_STEEPEST_EDGE,\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csc_matrix, vstack, issparse\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mapp√°k l√©trehoz√°sa\n",
    "def create_directories():\n",
    "    base_dir = 'combined_output'\n",
    "    subdirs = {\n",
    "        'combined_data': os.path.join(base_dir, 'combined_data'),\n",
    "        'predictions': os.path.join(base_dir, 'predictions'),\n",
    "        'feature_importance': os.path.join(base_dir, 'feature_importance'),\n",
    "        'learning_curves': os.path.join(base_dir, 'learning_curves'),\n",
    "        'confusion_matrices': os.path.join(base_dir, 'confusion_matrices'),\n",
    "        'results': os.path.join(base_dir, 'results')\n",
    "    }\n",
    "    for subdir in subdirs.values():\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "    return subdirs\n",
    "\n",
    "# Adatel≈ëk√©sz√≠t√©s\n",
    "def prepare_data():\n",
    "    data = pd.read_csv('animal_condition.csv')\n",
    "    print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "    data = data.dropna(subset=['Dangerous'])\n",
    "    data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "    valid_values = ['Yes', 'No']\n",
    "    data = data[data['Dangerous'].isin(valid_values)]\n",
    "    print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "    feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "    data[feature_cols] = data[feature_cols].fillna('Unknown')\n",
    "    X = pd.get_dummies(data[feature_cols], columns=feature_cols)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    # Hold-out set l√©trehoz√°sa (20%)\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    # Imbalanced tesztk√©szlet l√©trehoz√°sa (20% az X_orig-b√≥l)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "\n",
    "    # Kombin√°lt technik√°k defini√°l√°sa\n",
    "    combined_samplers = {\n",
    "        'smote_tomek': SMOTETomek(smote=SMOTE(sampling_strategy=0.5, random_state=42), tomek=TomekLinks(sampling_strategy='majority'), random_state=42),\n",
    "        'random_over_under': Pipeline([\n",
    "            ('oversample', RandomOverSampler(sampling_strategy=0.5, random_state=42)),\n",
    "            ('undersample', RandomUnderSampler(sampling_strategy=1.0, random_state=42))\n",
    "        ]),\n",
    "        'adasyn_nearmiss': Pipeline([\n",
    "            ('oversample', ADASYN(sampling_strategy=0.5, random_state=42)),\n",
    "            ('undersample', NearMiss(sampling_strategy=1.0, version=1))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    combined_data = {}\n",
    "    for name, sampler in combined_samplers.items():\n",
    "        print(f\"\\nApplying {name}...\")\n",
    "        X_res, y_res = sampler.fit_resample(X_train_val, y_train_val)\n",
    "        df = pd.DataFrame(X_res, columns=X.columns)\n",
    "        df['Dangerous'] = y_res.map({1: 'Yes', 0: 'No'})\n",
    "        # Ellen≈ërizz√ºk, hogy a c√©lf√°jl l√©tezik-e, √©s t√∂r√∂lj√ºk, ha igen\n",
    "        csv_path = os.path.join('combined_output/combined_data', f'combined_{name}.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            os.remove(csv_path)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Class distribution after {name}:\\n\", df['Dangerous'].value_counts())\n",
    "        combined_data[name] = df\n",
    "\n",
    "    return combined_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, X.columns, data\n",
    "\n",
    "# Mapp√°k l√©trehoz√°sa\n",
    "output_dirs = create_directories()\n",
    "\n",
    "# Bet√∂lt√©s\n",
    "combined_files = {\n",
    "    'smote_tomek': os.path.join(output_dirs['combined_data'], 'combined_smote_tomek.csv'),\n",
    "    'random_over_under': os.path.join(output_dirs['combined_data'], 'combined_random_over_under.csv'),\n",
    "    'adasyn_nearmiss': os.path.join(output_dirs['combined_data'], 'combined_adasyn_nearmiss.csv')\n",
    "}\n",
    "combined_data = {}\n",
    "X_holdout, y_holdout = None, None\n",
    "X_test_imbalanced, y_test_imbalanced = None, None\n",
    "feature_columns = None\n",
    "original_data = None\n",
    "\n",
    "if all(os.path.exists(file) for file in combined_files.values()):\n",
    "    print(\"Loading existing combined datasets...\")\n",
    "    for name, file in combined_files.items():\n",
    "        data = pd.read_csv(file)\n",
    "        # Ellen≈ërizz√ºk, hogy az oszlopok konzisztensek legyenek\n",
    "        original_data = pd.read_csv('animal_condition.csv')\n",
    "        original_data = original_data.dropna(subset=['Dangerous'])\n",
    "        original_data['Dangerous'] = original_data['Dangerous'].str.strip().str.capitalize()\n",
    "        original_data = original_data[original_data['Dangerous'].isin(['Yes', 'No'])]\n",
    "        feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "        original_data[feature_cols] = original_data[feature_cols].fillna('Unknown')\n",
    "        X_orig = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "        # Az adatk√©szlet m√°r one-hot encoded, csak biztos√≠tjuk, hogy az oszlopok megegyezzenek\n",
    "        data = data.reindex(columns=X_orig.columns, fill_value=0)\n",
    "        data['Dangerous'] = pd.read_csv(file)['Dangerous']\n",
    "        combined_data[name] = data\n",
    "        print(f\"Class distribution after loading {name}:\\n\", data['Dangerous'].value_counts())\n",
    "    X_orig_full = X_orig\n",
    "    y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X_orig_full, y_orig_full, test_size=0.2, random_state=42, stratify=y_orig_full)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    feature_columns = X_orig.columns\n",
    "else:\n",
    "    combined_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, feature_columns, original_data = prepare_data()\n",
    "\n",
    "# Modellek defini√°l√°sa\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100, max_depth=3, min_samples_split=15),\n",
    "    'logistic_regression': LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000, C=0.005),\n",
    "    'xgboost': XGBClassifier(scale_pos_weight=849/20, random_state=42, eval_metric='logloss', max_depth=3, reg_lambda=3, alpha=1)\n",
    "}\n",
    "\n",
    "# Tesztel√©s √©s √©rt√©kel√©s\n",
    "results = []\n",
    "learning_curve_data = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_orig_full = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_scaled = scaler_orig.fit_transform(X_orig_full)\n",
    "\n",
    "for sampler_name, data in combined_data.items():\n",
    "    print(f\"\\nProcessing {sampler_name} dataset...\")\n",
    "    X = data.drop('Dangerous', axis=1)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {sampler_name}...\")\n",
    "\n",
    "        cv_f1 = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        cv_f1_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='roc_auc')\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_scores = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
    "\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        # Konf√∫zi√≥s m√°trix sz√°m√≠t√°sa\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "        X_holdout_scaled = scaler.transform(X_holdout)\n",
    "        y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "        holdout_f1 = f1_score(y_holdout, y_holdout_pred, average='macro')\n",
    "        holdout_roc_auc = roc_auc_score(y_holdout, y_holdout_pred)\n",
    "\n",
    "        # Feature Importance ment√©se (csak fa alap√∫ modellekhez)\n",
    "        if model_name in ['random_forest', 'xgboost']:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            importance_file = f'feature_importance_{sampler_name}_{model_name}.csv'\n",
    "            target_path = os.path.join(output_dirs['feature_importance'], importance_file)\n",
    "            if os.path.exists(target_path):\n",
    "                os.remove(target_path)\n",
    "            feature_importance.to_csv(importance_file, index=False)\n",
    "\n",
    "        # Predikci√≥k ment√©se\n",
    "        pred_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_test_pred,\n",
    "            'proba_yes': y_test_scores\n",
    "        })\n",
    "        pred_file = f'predictions_{sampler_name}_{model_name}.csv'\n",
    "        target_path = os.path.join(output_dirs['predictions'], pred_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        pred_df.to_csv(pred_file, index=False)\n",
    "\n",
    "        result = {\n",
    "            'sampler': sampler_name,\n",
    "            'model': model_name,\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_orig': cv_f1_orig.mean(),\n",
    "            'test_f1': test_f1,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'holdout_roc_auc': holdout_roc_auc,\n",
    "            'confusion_matrices': cm.tolist()  # Konf√∫zi√≥s m√°trix t√°rol√°sa\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        train_sizes, train_scores, valid_scores = learning_curve(model, X_scaled, y, cv=skf, scoring='f1_macro', train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "        for size, tr_score, val_score in zip(train_sizes, train_scores.mean(axis=1), valid_scores.mean(axis=1)):\n",
    "            learning_curve_data.append({\n",
    "                'sampler': sampler_name,\n",
    "                'model': model_name,\n",
    "                'train_size': size,\n",
    "                'train_f1': tr_score,\n",
    "                'valid_f1': val_score\n",
    "            })\n",
    "\n",
    "        # Tanul√°si g√∂rbe √°bra ment√©se\n",
    "        curve_file = f'learning_curve_{sampler_name}_{model_name}.png'\n",
    "        target_path = os.path.join(output_dirs['learning_curves'], curve_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')\n",
    "        plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1')\n",
    "        plt.title(f'Learning Curve: {sampler_name} - {model_name}')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel('F1 Score (Macro)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(curve_file)\n",
    "        plt.close()\n",
    "\n",
    "# Kimeneti f√°jlok √°thelyez√©se a megfelel≈ë mapp√°kba\n",
    "for res in results:\n",
    "    sampler = res['sampler']\n",
    "    model = res['model']\n",
    "\n",
    "    # üîπ Predikci√≥k\n",
    "    pred_file = f'predictions_{sampler}_{model}.csv'\n",
    "    if os.path.exists(pred_file):\n",
    "        target_path = os.path.join(output_dirs['predictions'], pred_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(pred_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {pred_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # üîπ Feature importance\n",
    "    importance_file = f'feature_importance_{sampler}_{model}.csv'\n",
    "    if os.path.exists(importance_file):\n",
    "        target_path = os.path.join(output_dirs['feature_importance'], importance_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(importance_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {importance_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # üîπ Learning curve\n",
    "    curve_file = f'learning_curve_{sampler}_{model}.png'\n",
    "    if os.path.exists(curve_file):\n",
    "        target_path = os.path.join(output_dirs['learning_curves'], curve_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(curve_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {curve_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # üîπ Konf√∫zi√≥s m√°trix √°bra ment√©se\n",
    "    cm = np.array(res['confusion_matrices'])\n",
    "    cm_filename = f'conf_matrix_{sampler}_{model}.png'\n",
    "    target_path = os.path.join(output_dirs['confusion_matrices'], cm_filename)\n",
    "    if os.path.exists(target_path):\n",
    "        os.remove(target_path)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred: No', 'Pred: Yes'],\n",
    "                yticklabels=['Actual: No', 'Actual: Yes'])\n",
    "    plt.title(f'Confusion Matrix\\n{sampler} + {model}')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(target_path)\n",
    "    plt.close()\n",
    "\n",
    "# üìÑ V√©gs≈ë eredm√©nyek ment√©se mapp√°ba\n",
    "results_df = pd.DataFrame(results)\n",
    "results_csv_path = os.path.join(output_dirs['results'], 'classification_results.csv')\n",
    "results_json_path = os.path.join(output_dirs['results'], 'classification_results.json')\n",
    "learning_curve_path = os.path.join(output_dirs['results'], 'learning_curve_results.csv')\n",
    "\n",
    "if os.path.exists(results_csv_path):\n",
    "    os.remove(results_csv_path)\n",
    "if os.path.exists(results_json_path):\n",
    "    os.remove(results_json_path)\n",
    "if os.path.exists(learning_curve_path):\n",
    "    os.remove(learning_curve_path)\n",
    "\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "results_df.to_json(results_json_path, orient='records', lines=True)\n",
    "learning_curve_df = pd.DataFrame(learning_curve_data)\n",
    "learning_curve_df.to_csv(learning_curve_path, index=False)\n",
    "\n",
    "print(\"\\nüìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07540fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Dangerous: 0\n",
      "Original class distribution:\n",
      " Dangerous\n",
      "Yes    818\n",
      "No      20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying smote_tomek...\n",
      "Class distribution after smote_tomek:\n",
      " Dangerous\n",
      "Yes    523\n",
      "No     261\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying random_over_under...\n",
      "Class distribution after random_over_under:\n",
      " Dangerous\n",
      "No     261\n",
      "Yes    261\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying adasyn_nearmiss...\n",
      "Class distribution after adasyn_nearmiss:\n",
      " Dangerous\n",
      "No     260\n",
      "Yes    260\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing smote_tomek dataset...\n",
      "Training random_forest on smote_tomek...\n",
      "Training logistic_regression on smote_tomek...\n",
      "Training decision_tree on smote_tomek...\n",
      "\n",
      "Processing random_over_under dataset...\n",
      "Training random_forest on random_over_under...\n",
      "Training logistic_regression on random_over_under...\n",
      "Training decision_tree on random_over_under...\n",
      "\n",
      "Processing adasyn_nearmiss dataset...\n",
      "Training random_forest on adasyn_nearmiss...\n",
      "Training logistic_regression on adasyn_nearmiss...\n",
      "Training decision_tree on adasyn_nearmiss...\n",
      "Warning: predictions_smote_tomek_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_smote_tomek_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_smote_tomek_random_forest.png does not exist and cannot be moved.\n",
      "Warning: predictions_smote_tomek_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_smote_tomek_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_smote_tomek_logistic_regression.png does not exist and cannot be moved.\n",
      "Warning: predictions_smote_tomek_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_smote_tomek_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_smote_tomek_decision_tree.png does not exist and cannot be moved.\n",
      "Warning: predictions_random_over_under_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_random_over_under_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_random_over_under_random_forest.png does not exist and cannot be moved.\n",
      "Warning: predictions_random_over_under_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_random_over_under_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_random_over_under_logistic_regression.png does not exist and cannot be moved.\n",
      "Warning: predictions_random_over_under_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_random_over_under_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_random_over_under_decision_tree.png does not exist and cannot be moved.\n",
      "Warning: predictions_adasyn_nearmiss_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_adasyn_nearmiss_random_forest.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_adasyn_nearmiss_random_forest.png does not exist and cannot be moved.\n",
      "Warning: predictions_adasyn_nearmiss_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_adasyn_nearmiss_logistic_regression.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_adasyn_nearmiss_logistic_regression.png does not exist and cannot be moved.\n",
      "Warning: predictions_adasyn_nearmiss_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: feature_importance_adasyn_nearmiss_decision_tree.csv does not exist and cannot be moved.\n",
      "Warning: learning_curve_adasyn_nearmiss_decision_tree.png does not exist and cannot be moved.\n",
      "\n",
      "üìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mapp√°k l√©trehoz√°sa\n",
    "def create_directories():\n",
    "    base_dir = 'combined_output'\n",
    "    subdirs = {\n",
    "        'combined_data': os.path.join(base_dir, 'combined_data'),\n",
    "        'predictions': os.path.join(base_dir, 'predictions'),\n",
    "        'feature_importance': os.path.join(base_dir, 'feature_importance'),\n",
    "        'learning_curves': os.path.join(base_dir, 'learning_curves'),\n",
    "        'confusion_matrices': os.path.join(base_dir, 'confusion_matrices'),\n",
    "        'results': os.path.join(base_dir, 'results')\n",
    "    }\n",
    "    for subdir in subdirs.values():\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "    return subdirs\n",
    "\n",
    "# Adatel≈ëk√©sz√≠t√©s\n",
    "def prepare_data():\n",
    "    # Az el≈ëk√©sz√≠tett adatf√°jl bet√∂lt√©se\n",
    "    data = pd.read_csv('animal_condition_processed.csv')\n",
    "    print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "    data = data.dropna(subset=['Dangerous'])\n",
    "    data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "    valid_values = ['Yes', 'No']\n",
    "    data = data[data['Dangerous'].isin(valid_values)]\n",
    "    print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "    feature_cols = ['AnimalGroup', 'Animal', 'Species', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "    data[feature_cols] = data[feature_cols].fillna('unknown')\n",
    "    X = pd.get_dummies(data[feature_cols], columns=feature_cols).astype(int)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    # Hold-out set l√©trehoz√°sa (20%)\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    # Imbalanced tesztk√©szlet l√©trehoz√°sa (20% az X_orig-b√≥l)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "\n",
    "    # Kombin√°lt technik√°k defini√°l√°sa\n",
    "    combined_samplers = {\n",
    "        'smote_tomek': SMOTETomek(smote=SMOTE(sampling_strategy=0.5, random_state=42), tomek=TomekLinks(sampling_strategy='majority'), random_state=42),\n",
    "        'random_over_under': Pipeline([\n",
    "            ('oversample', RandomOverSampler(sampling_strategy=0.5, random_state=42)),\n",
    "            ('undersample', RandomUnderSampler(sampling_strategy=1.0, random_state=42))\n",
    "        ]),\n",
    "        'adasyn_nearmiss': Pipeline([\n",
    "            ('oversample', ADASYN(sampling_strategy=0.5, random_state=42)),\n",
    "            ('undersample', NearMiss(sampling_strategy=1.0, version=1))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    combined_data = {}\n",
    "    for name, sampler in combined_samplers.items():\n",
    "        print(f\"\\nApplying {name}...\")\n",
    "        X_res, y_res = sampler.fit_resample(X_train_val, y_train_val)\n",
    "        df = pd.DataFrame(X_res, columns=X.columns)\n",
    "        df['Dangerous'] = y_res.map({1: 'Yes', 0: 'No'})\n",
    "        # Ellen≈ërizz√ºk, hogy a c√©lf√°jl l√©tezik-e, √©s t√∂r√∂lj√ºk, ha igen\n",
    "        csv_path = os.path.join('combined_output/combined_data', f'combined_{name}.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            os.remove(csv_path)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Class distribution after {name}:\\n\", df['Dangerous'].value_counts())\n",
    "        combined_data[name] = df\n",
    "\n",
    "    return combined_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, X.columns, data\n",
    "\n",
    "# Mapp√°k l√©trehoz√°sa\n",
    "output_dirs = create_directories()\n",
    "\n",
    "# Bet√∂lt√©s\n",
    "combined_files = {\n",
    "    'smote_tomek': os.path.join(output_dirs['combined_data'], 'combined_smote_tomek.csv'),\n",
    "    'random_over_under': os.path.join(output_dirs['combined_data'], 'combined_random_over_under.csv'),\n",
    "    'adasyn_nearmiss': os.path.join(output_dirs['combined_data'], 'combined_adasyn_nearmiss.csv')\n",
    "}\n",
    "combined_data = {}\n",
    "X_holdout, y_holdout = None, None\n",
    "X_test_imbalanced, y_test_imbalanced = None, None\n",
    "feature_columns = None\n",
    "original_data = None\n",
    "\n",
    "if all(os.path.exists(file) for file in combined_files.values()):\n",
    "    print(\"Loading existing combined datasets...\")\n",
    "    for name, file in combined_files.items():\n",
    "        data = pd.read_csv(file)\n",
    "        # Ellen≈ërizz√ºk, hogy az oszlopok konzisztensek legyenek\n",
    "        original_data = pd.read_csv('animal_condition_processed.csv')\n",
    "        original_data = original_data.dropna(subset=['Dangerous'])\n",
    "        original_data['Dangerous'] = original_data['Dangerous'].str.strip().str.capitalize()\n",
    "        original_data = original_data[original_data['Dangerous'].isin(['Yes', 'No'])]\n",
    "        feature_cols = ['AnimalGroup', 'Animal', 'Species', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "        original_data[feature_cols] = original_data[feature_cols].fillna('unknown')\n",
    "        X_orig = pd.get_dummies(original_data[feature_cols], columns=feature_cols).astype(int)\n",
    "        # Az adatk√©szlet m√°r one-hot encoded, csak biztos√≠tjuk, hogy az oszlopok megegyezzenek\n",
    "        data = data.reindex(columns=X_orig.columns, fill_value=0)\n",
    "        data['Dangerous'] = pd.read_csv(file)['Dangerous']\n",
    "        combined_data[name] = data\n",
    "        print(f\"Class distribution after loading {name}:\\n\", data['Dangerous'].value_counts())\n",
    "    X_orig_full = X_orig\n",
    "    y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X_orig_full, y_orig_full, test_size=0.2, random_state=42, stratify=y_orig_full)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    feature_columns = X_orig.columns\n",
    "else:\n",
    "    combined_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, feature_columns, original_data = prepare_data()\n",
    "\n",
    "# Modellek defini√°l√°sa\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100, max_depth=3, min_samples_split=15),\n",
    "    'logistic_regression': LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000, C=0.005),\n",
    "    'decision_tree': DecisionTreeClassifier(class_weight='balanced', random_state=42, max_depth=3, min_samples_split=15)\n",
    "}\n",
    "\n",
    "# Tesztel√©s √©s √©rt√©kel√©s\n",
    "results = []\n",
    "learning_curve_data = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_orig_full = pd.get_dummies(original_data[feature_cols], columns=feature_cols).astype(int)\n",
    "y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_scaled = scaler_orig.fit_transform(X_orig_full)\n",
    "\n",
    "for sampler_name, data in combined_data.items():\n",
    "    print(f\"\\nProcessing {sampler_name} dataset...\")\n",
    "    X = data.drop('Dangerous', axis=1)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {sampler_name}...\")\n",
    "\n",
    "        cv_f1 = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        cv_f1_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='roc_auc')\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_scores = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
    "\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        # Konf√∫zi√≥s m√°trix sz√°m√≠t√°sa\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "        X_holdout_scaled = scaler.transform(X_holdout)\n",
    "        y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "        holdout_f1 = f1_score(y_holdout, y_holdout_pred, average='macro')\n",
    "        holdout_roc_auc = roc_auc_score(y_holdout, y_holdout_pred)\n",
    "\n",
    "        # Feature Importance ment√©se (csak fa alap√∫ modellekhez)\n",
    "        if model_name in ['random_forest', 'decision_tree']:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            importance_file = f'feature_importance_{sampler_name}_{model_name}.csv'\n",
    "            target_path = os.path.join(output_dirs['feature_importance'], importance_file)\n",
    "            if os.path.exists(target_path):\n",
    "                os.remove(target_path)\n",
    "            feature_importance.to_csv(target_path, index=False)\n",
    "\n",
    "        # Predikci√≥k ment√©se\n",
    "        pred_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_test_pred,\n",
    "            'proba_yes': y_test_scores\n",
    "        })\n",
    "        pred_file = f'predictions_{sampler_name}_{model_name}.csv'\n",
    "        target_path = os.path.join(output_dirs['predictions'], pred_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        pred_df.to_csv(target_path, index=False)\n",
    "\n",
    "        result = {\n",
    "            'sampler': sampler_name,\n",
    "            'model': model_name,\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_orig': cv_f1_orig.mean(),\n",
    "            'test_f1': test_f1,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'holdout_roc_auc': holdout_roc_auc,\n",
    "            'confusion_matrices': cm.tolist()\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        train_sizes, train_scores, valid_scores = learning_curve(model, X_scaled, y, cv=skf, scoring='f1_macro', train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "        for size, tr_score, val_score in zip(train_sizes, train_scores.mean(axis=1), valid_scores.mean(axis=1)):\n",
    "            learning_curve_data.append({\n",
    "                'sampler': sampler_name,\n",
    "                'model': model_name,\n",
    "                'train_size': size,\n",
    "                'train_f1': tr_score,\n",
    "                'valid_f1': val_score\n",
    "            })\n",
    "\n",
    "        # Tanul√°si g√∂rbe √°bra ment√©se\n",
    "        curve_file = f'learning_curve_{sampler_name}_{model_name}.png'\n",
    "        target_path = os.path.join(output_dirs['learning_curves'], curve_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')\n",
    "        plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1')\n",
    "        plt.title(f'Learning Curve: {sampler_name} - {model_name}')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel('F1 Score (Macro)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(target_path)\n",
    "        plt.close()\n",
    "\n",
    "# Kimeneti f√°jlok √°thelyez√©se a megfelel≈ë mapp√°kba\n",
    "for res in results:\n",
    "    sampler = res['sampler']\n",
    "    model = res['model']\n",
    "\n",
    "    # Predikci√≥k\n",
    "    pred_file = f'predictions_{sampler}_{model}.csv'\n",
    "    if os.path.exists(pred_file):\n",
    "        target_path = os.path.join(output_dirs['predictions'], pred_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(pred_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {pred_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # Feature importance\n",
    "    importance_file = f'feature_importance_{sampler}_{model}.csv'\n",
    "    if os.path.exists(importance_file):\n",
    "        target_path = os.path.join(output_dirs['feature_importance'], importance_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(importance_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {importance_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # Learning curve\n",
    "    curve_file = f'learning_curve_{sampler}_{model}.png'\n",
    "    if os.path.exists(curve_file):\n",
    "        target_path = os.path.join(output_dirs['learning_curves'], curve_file)\n",
    "        if os.path.exists(target_path):\n",
    "            os.remove(target_path)\n",
    "        os.rename(curve_file, target_path)\n",
    "    else:\n",
    "        print(f\"Warning: {curve_file} does not exist and cannot be moved.\")\n",
    "\n",
    "    # Konf√∫zi√≥s m√°trix √°bra ment√©se\n",
    "    cm = np.array(res['confusion_matrices'])\n",
    "    cm_filename = f'conf_matrix_{sampler}_{model}.png'\n",
    "    target_path = os.path.join(output_dirs['confusion_matrices'], cm_filename)\n",
    "    if os.path.exists(target_path):\n",
    "        os.remove(target_path)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred: No', 'Pred: Yes'],\n",
    "                yticklabels=['Actual: No', 'Actual: Yes'])\n",
    "    plt.title(f'Confusion Matrix\\n{sampler} + {model}')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(target_path)\n",
    "    plt.close()\n",
    "\n",
    "# V√©gs≈ë eredm√©nyek ment√©se mapp√°ba\n",
    "results_df = pd.DataFrame(results)\n",
    "results_csv_path = os.path.join(output_dirs['results'], 'classification_results.csv')\n",
    "results_json_path = os.path.join(output_dirs['results'], 'classification_results.json')\n",
    "learning_curve_path = os.path.join(output_dirs['results'], 'learning_curve_results.csv')\n",
    "\n",
    "if os.path.exists(results_csv_path):\n",
    "    os.remove(results_csv_path)\n",
    "if os.path.exists(results_json_path):\n",
    "    os.remove(results_json_path)\n",
    "if os.path.exists(learning_curve_path):\n",
    "    os.remove(learning_curve_path)\n",
    "\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "results_df.to_json(results_json_path, orient='records', lines=True)\n",
    "learning_curve_df = pd.DataFrame(learning_curve_data)\n",
    "learning_curve_df.to_csv(learning_curve_path, index=False)\n",
    "\n",
    "print(\"\\nüìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing combined datasets...\n",
      "Class distribution after loading smote_tomek:\n",
      " Dangerous\n",
      "Yes    543\n",
      "No     271\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading random_over_under:\n",
      " Dangerous\n",
      "No     271\n",
      "Yes    271\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading adasyn_nearmiss:\n",
      " Dangerous\n",
      "No     267\n",
      "Yes    267\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing smote_tomek dataset...\n",
      "Training random_forest on smote_tomek with F1 Early Stopping...\n",
      "Epoch 1: Validation F1 = 0.4899\n",
      "Epoch 2: Validation F1 = 0.6902\n",
      "Epoch 3: Validation F1 = 0.7493\n",
      "Epoch 4: Validation F1 = 0.8740\n",
      "Epoch 5: Validation F1 = 0.8565\n",
      "Epoch 6: Validation F1 = 0.8905\n",
      "Epoch 7: Validation F1 = 0.9196\n",
      "Epoch 8: Validation F1 = 0.9290\n",
      "Epoch 9: Validation F1 = 0.9290\n",
      "Epoch 10: Validation F1 = 0.9290\n",
      "Epoch 11: Validation F1 = 0.9290\n",
      "Epoch 12: Validation F1 = 0.9100\n",
      "Epoch 13: Validation F1 = 0.9196\n",
      "Early stopping triggered after 13 trees.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'F1EarlyStopping' object has no attribute 'best_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 216\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         model\u001b[38;5;241m.\u001b[39mn_estimators \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 216\u001b[0m     model \u001b[38;5;241m=\u001b[39m f1_callback\u001b[38;5;241m.\u001b[39mbest_model \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mf1_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# Incremental training for LogisticRegression\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'F1EarlyStopping' object has no attribute 'best_model'"
     ]
    }
   ],
   "source": [
    "# üì¶ Alap k√∂nyvt√°rak\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# üìà Modellek √©s sk√°l√°z√°s\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "\n",
    "# ‚öñÔ∏è Imbalanced data kezel√©s\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# üå≥ XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# üîï Warnings kikapcsol√°sa\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b85cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908d7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
