{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c83279",
   "metadata": {},
   "source": [
    "## Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e20b707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Dangerous: 2\n",
      "Original class distribution:\n",
      " Dangerous\n",
      "Yes    849\n",
      "No      20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying random_oversampling...\n",
      "Class distribution after random_oversampling:\n",
      " Dangerous\n",
      "Yes    849\n",
      "No     849\n",
      "Name: count, dtype: int64\n",
      "Saved oversampled data to oversampled_random_oversampling.csv, .json, and .parquet\n",
      "\n",
      "Applying smote...\n",
      "Class distribution after smote:\n",
      " Dangerous\n",
      "Yes    849\n",
      "No     849\n",
      "Name: count, dtype: int64\n",
      "Saved oversampled data to oversampled_smote.csv, .json, and .parquet\n",
      "\n",
      "Applying adasyn...\n",
      "Class distribution after adasyn:\n",
      " Dangerous\n",
      "Yes    849\n",
      "No     840\n",
      "Name: count, dtype: int64\n",
      "Saved oversampled data to oversampled_adasyn.csv, .json, and .parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adatok bet√∂lt√©se\n",
    "data = pd.read_csv('animal_condition.csv')\n",
    "\n",
    "# Hi√°nyz√≥ √©rt√©kek kezel√©se\n",
    "print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "data = data.dropna(subset=['Dangerous'])\n",
    "data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "valid_values = ['Yes', 'No']\n",
    "data = data[data['Dangerous'].isin(valid_values)]\n",
    "print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "# Jellemz≈ëk √©s c√©lv√°ltoz√≥\n",
    "feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "if data[feature_cols].isnull().sum().any():\n",
    "    data[feature_cols] = data[feature_cols].fillna('Unknown')\n",
    "\n",
    "# Eredeti kategorikus √©rt√©kek ment√©se\n",
    "data_original = data.copy()\n",
    "\n",
    "# Kategorikus jellemz≈ëk k√≥dol√°sa\n",
    "le_dict = {}\n",
    "for col in feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "le_dangerous = LabelEncoder()\n",
    "data['Dangerous'] = le_dangerous.fit_transform(data['Dangerous'].map({'Yes': 1, 'No': 0}))\n",
    "\n",
    "# Jellemz≈ëk √©s c√©lv√°ltoz√≥ sz√©tv√°laszt√°sa\n",
    "X = data[feature_cols]\n",
    "y = data['Dangerous']\n",
    "\n",
    "# T√∫lmintav√©telez√©si technik√°k\n",
    "oversamplers = {\n",
    "    'random_oversampling': RandomOverSampler(random_state=42),\n",
    "    'smote': SMOTE(random_state=42),\n",
    "    'adasyn': ADASYN(random_state=42)\n",
    "}\n",
    "\n",
    "# Eredm√©nyek t√°rol√°sa\n",
    "for name, oversampler in oversamplers.items():\n",
    "    print(f\"\\nApplying {name}...\")\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "    \n",
    "    # Visszaalak√≠t√°s kategorikus √©rt√©kekre\n",
    "    resampled_data = pd.DataFrame(X_resampled, columns=feature_cols)\n",
    "    for col in feature_cols:\n",
    "        resampled_data[col] = le_dict[col].inverse_transform(resampled_data[col].astype(int))\n",
    "    resampled_data['Dangerous'] = le_dangerous.inverse_transform(y_resampled)\n",
    "    resampled_data['Dangerous'] = resampled_data['Dangerous'].map({1: 'Yes', 0: 'No'})\n",
    "    \n",
    "    # Oszt√°lyeloszl√°s ki√≠r√°sa\n",
    "    print(f\"Class distribution after {name}:\\n\", resampled_data['Dangerous'].value_counts())\n",
    "    \n",
    "    # Ment√©s k√ºl√∂nb√∂z≈ë form√°tumokban\n",
    "    resampled_data.to_csv(f'oversampled_{name}.csv', index=False)\n",
    "    resampled_data.to_json(f'oversampled_{name}.json', orient='records', lines=True)\n",
    "    resampled_data.to_parquet(f'oversampled_{name}.parquet', index=False)\n",
    "    print(f\"Saved oversampled data to oversampled_{name}.csv, .json, and .parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfac0f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing random_oversampling dataset...\n",
      "Training random_forest on random_oversampling...\n",
      "Feature Importance for random_oversampling - random_forest:\n",
      "                                  Feature  Importance\n",
      "546                   symptoms3_Diarrhea    0.052586\n",
      "4                     AnimalName_Chicken    0.042808\n",
      "819          symptoms4_Labored breathing    0.040455\n",
      "472   symptoms2_Yellow or green dropping    0.039445\n",
      "245                   symptoms1_Weakness    0.035639\n",
      "1021         symptoms5_Greenish diarrhea    0.031373\n",
      "651                symptoms3_Slow growth    0.030415\n",
      "911                    symptoms4_Wattles    0.028413\n",
      "916                symptoms4_Weight loss    0.027768\n",
      "306                 symptoms2_Depression    0.027276\n",
      "Training logistic_regression on random_oversampling...\n",
      "Training xgboost on random_oversampling...\n",
      "Feature Importance for random_oversampling - xgboost:\n",
      "                         Feature  Importance\n",
      "191   symptoms1_Severe swellimg    0.345018\n",
      "4            AnimalName_Chicken    0.216001\n",
      "2          AnimalName_Buffaloes    0.065622\n",
      "263  symptoms1_loss of appetite    0.045095\n",
      "349         symptoms2_Hepatitis    0.042076\n",
      "93              symptoms1_Edema    0.039989\n",
      "326         symptoms2_Epistaxis    0.037154\n",
      "620     symptoms3_Paralyzed leg    0.029203\n",
      "245          symptoms1_Weakness    0.028094\n",
      "546          symptoms3_Diarrhea    0.025730\n",
      "\n",
      "Processing smote dataset...\n",
      "Training random_forest on smote...\n",
      "Feature Importance for smote - random_forest:\n",
      "                           Feature  Importance\n",
      "3               AnimalName_Cattle    0.066063\n",
      "4              AnimalName_Chicken    0.063121\n",
      "2            AnimalName_Buffaloes    0.041916\n",
      "109               symptoms1_Fever    0.040923\n",
      "36                 AnimalName_cat    0.033920\n",
      "1021  symptoms5_Greenish diarrhea    0.031169\n",
      "25                 AnimalName_Pig    0.030347\n",
      "12                AnimalName_Fowl    0.029741\n",
      "685            symptoms3_Vomiting    0.028618\n",
      "346     symptoms2_Heavy Breathing    0.026749\n",
      "Training logistic_regression on smote...\n",
      "Training xgboost on smote...\n",
      "Feature Importance for smote - xgboost:\n",
      "                           Feature  Importance\n",
      "4              AnimalName_Chicken    0.073809\n",
      "93                symptoms1_Edema    0.052007\n",
      "3               AnimalName_Cattle    0.050086\n",
      "346     symptoms2_Heavy Breathing    0.046964\n",
      "1021  symptoms5_Greenish diarrhea    0.042220\n",
      "326           symptoms2_Epistaxis    0.041731\n",
      "2            AnimalName_Buffaloes    0.041117\n",
      "109               symptoms1_Fever    0.038006\n",
      "958              symptoms5_Anemia    0.031068\n",
      "306          symptoms2_Depression    0.027890\n",
      "\n",
      "Processing adasyn dataset...\n",
      "Training random_forest on adasyn...\n",
      "Feature Importance for adasyn - random_forest:\n",
      "                     Feature  Importance\n",
      "109         symptoms1_Fever    0.058458\n",
      "3         AnimalName_Cattle    0.049540\n",
      "4        AnimalName_Chicken    0.049247\n",
      "685      symptoms3_Vomiting    0.048187\n",
      "2      AnimalName_Buffaloes    0.047908\n",
      "822      symptoms4_Lethargy    0.034590\n",
      "245      symptoms1_Weakness    0.033688\n",
      "25           AnimalName_Pig    0.031220\n",
      "1122  symptoms5_Weight loss    0.030225\n",
      "1064        symptoms5_Pains    0.029233\n",
      "Training logistic_regression on adasyn...\n",
      "Training xgboost on adasyn...\n",
      "Feature Importance for adasyn - xgboost:\n",
      "                        Feature  Importance\n",
      "4           AnimalName_Chicken    0.073031\n",
      "3            AnimalName_Cattle    0.051885\n",
      "346  symptoms2_Heavy Breathing    0.036472\n",
      "2         AnimalName_Buffaloes    0.036301\n",
      "109            symptoms1_Fever    0.036148\n",
      "158   symptoms1_Neck paralysis    0.031424\n",
      "245         symptoms1_Weakness    0.030467\n",
      "479         symptoms2_diarrhea    0.027929\n",
      "306       symptoms2_Depression    0.025722\n",
      "326        symptoms2_Epistaxis    0.025144\n",
      "\n",
      "Classification Results with Holdout and Imbalanced Test:\n",
      "           oversampler                model  cv_f1_mean  cv_f1_orig   test_f1  \\\n",
      "0  random_oversampling        random_forest    0.984501    0.795834  1.000000   \n",
      "1  random_oversampling  logistic_regression    0.990420    0.932164  0.997059   \n",
      "2  random_oversampling              xgboost    0.999265    0.615044  1.000000   \n",
      "3                smote        random_forest    0.887170    0.795834  0.941174   \n",
      "4                smote  logistic_regression    0.899784    0.932164  0.961756   \n",
      "5                smote              xgboost    0.760162    0.615044  0.782207   \n",
      "6               adasyn        random_forest    0.874118    0.795834  0.908219   \n",
      "7               adasyn  logistic_regression    0.898549    0.932164  0.973361   \n",
      "8               adasyn              xgboost    0.760816    0.615044  0.765960   \n",
      "\n",
      "     f1_gap  test_roc_auc  roc_auc_gap    pr_auc     no_f1  \\\n",
      "0  0.000000      1.000000     0.000000  1.000000  1.000000   \n",
      "1  0.001468      0.997059     0.001468  1.000000  0.997067   \n",
      "2  0.000000      1.000000     0.000000  1.000000  1.000000   \n",
      "3 -0.028068      0.941176    -0.028069  0.982351  0.941520   \n",
      "4  0.035298      0.961765     0.035290  0.996410  0.962319   \n",
      "5  0.011618      0.791176     0.010738  0.990339  0.738007   \n",
      "6 -0.015548      0.908158    -0.015447  0.969296  0.905775   \n",
      "7  0.025158      0.973284     0.025235  0.991369  0.972810   \n",
      "8  0.029602      0.776786     0.026042  0.991673  0.712644   \n",
      "\n",
      "   test_imbalanced_f1  test_imbalanced_no_f1  holdout_f1  holdout_no_f1  \n",
      "0            1.000000               1.000000    1.000000       1.000000  \n",
      "1            0.926726               0.857143    0.942970       0.888889  \n",
      "2            1.000000               1.000000    1.000000       1.000000  \n",
      "3            0.613674               0.285714    0.670599       0.380952  \n",
      "4            0.827757               0.666667    1.000000       1.000000  \n",
      "5            1.000000               1.000000    1.000000       1.000000  \n",
      "6            0.569278               0.222222    0.670599       0.380952  \n",
      "7            0.926726               0.857143    1.000000       1.000000  \n",
      "8            1.000000               1.000000    1.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adatok el≈ëk√©sz√≠t√©se\n",
    "def prepare_data():\n",
    "    data = pd.read_csv('animal_condition.csv')\n",
    "    print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "    data = data.dropna(subset=['Dangerous'])\n",
    "    data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "    valid_values = ['Yes', 'No']\n",
    "    data = data[data['Dangerous'].isin(valid_values)]\n",
    "    print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "    feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "    data[feature_cols] = data[feature_cols].fillna('Unknown')\n",
    "\n",
    "    # One-Hot Encoding\n",
    "    X = pd.get_dummies(data[feature_cols], columns=feature_cols)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Hold-out set l√©trehoz√°sa (20%)\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # K√ºl√∂n tesztk√©szlet az eredeti adaton (20%)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    \n",
    "    oversamplers = {\n",
    "        'random_oversampling': RandomOverSampler(sampling_strategy=200/679, random_state=42),  # 679 = 764 * 0.8\n",
    "        'smote': SMOTE(sampling_strategy=200/679, random_state=42),\n",
    "        'adasyn': ADASYN(sampling_strategy=200/679, random_state=42)\n",
    "    }\n",
    "    \n",
    "    oversampled_data = {}\n",
    "    for name, oversampler in oversamplers.items():\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X_train_val, y_train_val)\n",
    "        resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        resampled_data['Dangerous'] = y_resampled.map({1: 'Yes', 0: 'No'})\n",
    "        oversampled_data[name] = resampled_data\n",
    "        resampled_data.to_csv(f'oversampled_{name}.csv', index=False)\n",
    "        print(f\"Class distribution after {name}:\\n\", resampled_data['Dangerous'].value_counts())\n",
    "    return oversampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, X.columns, data\n",
    "\n",
    "# Adatk√©szletek bet√∂lt√©se vagy l√©trehoz√°sa\n",
    "oversampled_files = {\n",
    "    'random_oversampling': 'oversampled_random_oversampling.csv',\n",
    "    'smote': 'oversampled_smote.csv',\n",
    "    'adasyn': 'oversampled_adasyn.csv'\n",
    "}\n",
    "oversampled_data = {}\n",
    "X_holdout, y_holdout = None, None\n",
    "X_test_imbalanced, y_test_imbalanced = None, None\n",
    "feature_columns = None\n",
    "original_data = None\n",
    "if all(os.path.exists(file) for file in oversampled_files.values()):\n",
    "    for name, file in oversampled_files.items():\n",
    "        data = pd.read_csv(file)\n",
    "        feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "        X = pd.get_dummies(data[feature_cols], columns=feature_cols)\n",
    "        original_data = pd.read_csv('animal_condition.csv')\n",
    "        original_data = original_data.dropna(subset=['Dangerous'])\n",
    "        original_data['Dangerous'] = original_data['Dangerous'].str.strip().str.capitalize()\n",
    "        original_data = original_data[original_data['Dangerous'].isin(['Yes', 'No'])]\n",
    "        original_data[feature_cols] = original_data[feature_cols].fillna('Unknown')\n",
    "        X_orig = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "        X = X.reindex(columns=X_orig.columns, fill_value=0)\n",
    "        data = X.copy()\n",
    "        data['Dangerous'] = pd.read_csv(file)['Dangerous']\n",
    "        oversampled_data[name] = data\n",
    "    X_orig_full = X_orig\n",
    "    y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X_orig_full, y_orig_full, test_size=0.2, random_state=42, stratify=y_orig_full)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    feature_columns = X_orig.columns\n",
    "else:\n",
    "    oversampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, feature_columns, original_data = prepare_data()\n",
    "\n",
    "# Modellek\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(\n",
    "        class_weight='balanced', random_state=42, n_estimators=100, max_depth=3, min_samples_split=15\n",
    "    ),\n",
    "    'logistic_regression': LogisticRegression(\n",
    "        class_weight='balanced', random_state=42, max_iter=1000, C=0.005\n",
    "    ),\n",
    "    'xgboost': XGBClassifier(\n",
    "        scale_pos_weight=849/20, random_state=42, eval_metric='logloss', max_depth=3, reg_lambda=3, alpha=1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Eredm√©nyek √©s predikci√≥k t√°rol√°sa\n",
    "results = []\n",
    "predictions = []\n",
    "learning_curve_data = []\n",
    "\n",
    "# Keresztvalid√°ci√≥ az eredeti adaton\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_orig_full = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_scaled = scaler_orig.fit_transform(X_orig_full)\n",
    "\n",
    "# Keresztvalid√°ci√≥ √©s tesztel√©s\n",
    "for oversampler_name, data in oversampled_data.items():\n",
    "    print(f\"\\nProcessing {oversampler_name} dataset...\")\n",
    "    \n",
    "    X = data.drop('Dangerous', axis=1)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {oversampler_name}...\")\n",
    "        \n",
    "        # Keresztvalid√°ci√≥ a t√∫lmintav√©telezett adaton\n",
    "        cv_f1 = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        # Keresztvalid√°ci√≥ az eredeti imbalanced adaton\n",
    "        cv_f1_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "        \n",
    "        try:\n",
    "            y_test_scores = model.predict_proba(X_test)[:, 1]\n",
    "        except AttributeError:\n",
    "            y_test_scores = model.predict(X_test)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        # Imbalanced tesztk√©szlet teljes√≠tm√©ny\n",
    "        X_test_imbalanced_scaled = scaler.transform(X_test_imbalanced)\n",
    "        y_test_imbalanced_pred = model.predict(X_test_imbalanced_scaled)\n",
    "        test_imbalanced_f1 = f1_score(y_test_imbalanced, y_test_imbalanced_pred, average='macro')\n",
    "        test_imbalanced_roc_auc = roc_auc_score(y_test_imbalanced, y_test_imbalanced_pred)\n",
    "        test_imbalanced_report = classification_report(y_test_imbalanced, y_test_imbalanced_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        \n",
    "        # Hold-out set teljes√≠tm√©ny\n",
    "        X_holdout_scaled = scaler.transform(X_holdout)\n",
    "        y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "        holdout_f1 = f1_score(y_holdout, y_holdout_pred, average='macro')\n",
    "        holdout_roc_auc = roc_auc_score(y_holdout, y_holdout_pred)\n",
    "        holdout_report = classification_report(y_holdout, y_holdout_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        \n",
    "        f1_gap = train_f1 - test_f1\n",
    "        roc_auc_gap = train_roc_auc - test_roc_auc\n",
    "        \n",
    "        report = classification_report(y_test, y_test_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "        \n",
    "        if model_name in ['random_forest', 'xgboost']:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            print(f\"Feature Importance for {oversampler_name} - {model_name}:\\n\", feature_importance.head(10))\n",
    "            feature_importance.to_csv(f'feature_importance_{oversampler_name}_{model_name}.csv', index=False)\n",
    "        \n",
    "        result = {\n",
    "            'oversampler': oversampler_name,\n",
    "            'model': model_name,\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_std': cv_f1.std(),\n",
    "            'cv_roc_auc_mean': cv_roc_auc.mean(),\n",
    "            'cv_roc_auc_std': cv_roc_auc.std(),\n",
    "            'cv_f1_orig': cv_f1_orig.mean(),\n",
    "            'cv_roc_auc_orig': cv_roc_auc_orig.mean(),\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'f1_gap': f1_gap,\n",
    "            'train_roc_auc': train_roc_auc,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'roc_auc_gap': roc_auc_gap,\n",
    "            'pr_auc': pr_auc,\n",
    "            'no_precision': report['No']['precision'],\n",
    "            'no_recall': report['No']['recall'],\n",
    "            'no_f1': report['No']['f1-score'],\n",
    "            'yes_precision': report['Yes']['precision'],\n",
    "            'yes_recall': report['Yes']['recall'],\n",
    "            'yes_f1': report['Yes']['f1-score'],\n",
    "            'test_imbalanced_f1': test_imbalanced_f1,\n",
    "            'test_imbalanced_roc_auc': test_imbalanced_roc_auc,\n",
    "            'test_imbalanced_no_f1': test_imbalanced_report['No']['f1-score'],\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'holdout_roc_auc': holdout_roc_auc,\n",
    "            'holdout_no_f1': holdout_report['No']['f1-score'],\n",
    "            'confusion_matrix': cm.tolist()\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        pred_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_test_pred,\n",
    "            'proba_yes': y_test_scores\n",
    "        })\n",
    "        pred_df.to_csv(f'predictions_{oversampler_name}_{model_name}.csv', index=False)\n",
    "        \n",
    "        train_sizes, train_scores, valid_scores = learning_curve(\n",
    "            model, X_scaled, y, cv=skf, scoring='f1_macro', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "        )\n",
    "        for size, train_score, valid_score in zip(train_sizes, train_scores.mean(axis=1), valid_scores.mean(axis=1)):\n",
    "            learning_curve_data.append({\n",
    "                'oversampler': oversampler_name,\n",
    "                'model': model_name,\n",
    "                'train_size': size,\n",
    "                'train_f1': train_score,\n",
    "                'valid_f1': valid_score,\n",
    "                'f1_gap': train_score - valid_score\n",
    "            })\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')\n",
    "        plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1')\n",
    "        plt.title(f'Learning Curve: {oversampler_name} - {model_name}')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel('F1 Score (Macro)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'learning_curve_{oversampler_name}_{model_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Eredm√©nyek ment√©se\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "results_df.to_json('classification_results.json', orient='records', lines=True)\n",
    "learning_curve_df = pd.DataFrame(learning_curve_data)\n",
    "learning_curve_df.to_csv('learning_curve_results.csv', index=False)\n",
    "\n",
    "# Eredm√©nyek ki√≠r√°sa\n",
    "print(\"\\nClassification Results with Holdout and Imbalanced Test:\")\n",
    "print(results_df[['oversampler', 'model', 'cv_f1_mean', 'cv_f1_orig', 'test_f1', 'f1_gap', 'test_roc_auc', 'roc_auc_gap', 'pr_auc', 'no_f1', 'test_imbalanced_f1', 'test_imbalanced_no_f1', 'holdout_f1', 'holdout_no_f1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10dc18bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing oversampled datasets...\n",
      "Class distribution after loading random_oversampling:\n",
      " Dangerous\n",
      "Yes    543\n",
      "No     543\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading smote:\n",
      " Dangerous\n",
      "Yes    543\n",
      "No     543\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading adasyn:\n",
      " Dangerous\n",
      "No     546\n",
      "Yes    543\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing random_oversampling dataset...\n",
      "Training random_forest on random_oversampling...\n",
      "Feature Importance for random_oversampling - random_forest:\n",
      "                          Feature  Importance\n",
      "7                  Animal_cattle    0.063824\n",
      "168     symptoms1_neck paralysis    0.062046\n",
      "22             Species_buffaloes    0.058139\n",
      "653   symptoms3_ruffled feathers    0.036683\n",
      "38                 Species_sheep    0.035910\n",
      "262           symptoms1_weakness    0.033116\n",
      "1109          symptoms5_weakness    0.030241\n",
      "93      symptoms1_drooping wings    0.027935\n",
      "666        symptoms3_slow growth    0.026036\n",
      "1014            symptoms5_lesion    0.025991\n",
      "Training logistic_regression on random_oversampling...\n",
      "Training decision_tree on random_oversampling...\n",
      "Feature Importance for random_oversampling - decision_tree:\n",
      "                                 Feature  Importance\n",
      "23                      Species_chicken    0.453446\n",
      "22                    Species_buffaloes    0.235401\n",
      "13                        Animal_monkey    0.234322\n",
      "530                  symptoms3_coughing    0.038382\n",
      "1112              symptoms5_weight loss    0.032631\n",
      "96                      symptoms1_edema    0.005818\n",
      "743                 symptoms4_deability    0.000000\n",
      "747   symptoms4_defienciency in vitamin    0.000000\n",
      "746   symptoms4_decresed egg production    0.000000\n",
      "745             symptoms4_death of lamb    0.000000\n",
      "\n",
      "Processing smote dataset...\n",
      "Training random_forest on smote...\n",
      "Feature Importance for smote - random_forest:\n",
      "                 Feature  Importance\n",
      "7         Animal_cattle    0.086935\n",
      "22    Species_buffaloes    0.077501\n",
      "41      Species_unknown    0.065675\n",
      "38        Species_sheep    0.059004\n",
      "116     symptoms1_fever    0.048731\n",
      "26         Species_duck    0.034810\n",
      "23      Species_chicken    0.030522\n",
      "14   Animal_other birds    0.029845\n",
      "5        Animal_caprine    0.027070\n",
      "1    AnimalGroup_mammal    0.026818\n",
      "Training logistic_regression on smote...\n",
      "Training decision_tree on smote...\n",
      "Feature Importance for smote - decision_tree:\n",
      "                                Feature  Importance\n",
      "41                     Species_unknown    0.417096\n",
      "116                    symptoms1_fever    0.258585\n",
      "5                       Animal_caprine    0.193154\n",
      "351                symptoms2_hepatitis    0.070611\n",
      "13                       Animal_monkey    0.047992\n",
      "530                 symptoms3_coughing    0.012562\n",
      "0                     AnimalGroup_bird    0.000000\n",
      "748              symptoms4_dehydration    0.000000\n",
      "747  symptoms4_defienciency in vitamin    0.000000\n",
      "746  symptoms4_decresed egg production    0.000000\n",
      "\n",
      "Processing adasyn dataset...\n",
      "Training random_forest on adasyn...\n",
      "Feature Importance for adasyn - random_forest:\n",
      "                 Feature  Importance\n",
      "22    Species_buffaloes    0.070421\n",
      "41      Species_unknown    0.060234\n",
      "38        Species_sheep    0.060175\n",
      "5        Animal_caprine    0.053476\n",
      "116     symptoms1_fever    0.052754\n",
      "28         Species_fowl    0.042781\n",
      "7         Animal_cattle    0.041968\n",
      "1049    symptoms5_pains    0.040925\n",
      "26         Species_duck    0.030927\n",
      "15           Animal_pig    0.030576\n",
      "Training logistic_regression on adasyn...\n",
      "Training decision_tree on adasyn...\n",
      "Feature Importance for adasyn - decision_tree:\n",
      "                             Feature  Importance\n",
      "41                  Species_unknown    0.368332\n",
      "116                 symptoms1_fever    0.269988\n",
      "5                    Animal_caprine    0.190648\n",
      "13                    Animal_monkey    0.094019\n",
      "351             symptoms2_hepatitis    0.077012\n",
      "738   symptoms4_consistency of milk    0.000000\n",
      "739  symptoms4_cornea become cloudy    0.000000\n",
      "740              symptoms4_coughing    0.000000\n",
      "741                symptoms4_cramps    0.000000\n",
      "742  symptoms4_crusting of the skin    0.000000\n",
      "\n",
      "Classification Results with Holdout and Imbalanced Test:\n",
      "           oversampler                model  cv_f1_mean  cv_f1_orig   test_f1  \\\n",
      "0  random_oversampling        random_forest    0.934344    0.783296  0.880694   \n",
      "1  random_oversampling  logistic_regression    0.974653    0.932164  0.972456   \n",
      "2  random_oversampling        decision_tree    0.923444    0.565093  0.930865   \n",
      "3                smote        random_forest    0.893467    0.783296  0.848544   \n",
      "4                smote  logistic_regression    0.910630    0.932164  0.907478   \n",
      "5                smote        decision_tree    0.836021    0.565093  0.854863   \n",
      "6               adasyn        random_forest    0.899645    0.783296  0.917320   \n",
      "7               adasyn  logistic_regression    0.891887    0.932164  0.916864   \n",
      "8               adasyn        decision_tree    0.837479    0.565093  0.835684   \n",
      "\n",
      "     f1_gap  test_roc_auc  roc_auc_gap    pr_auc     no_f1  \\\n",
      "0  0.014406      0.880734     0.014427  0.983249  0.878505   \n",
      "1  0.026392      0.972477     0.026371  0.999519  0.973214   \n",
      "2  0.002015      0.931193     0.001987  0.969323  0.935622   \n",
      "3  0.013207      0.848624     0.013127  0.955376  0.845070   \n",
      "4  0.092522      0.908257     0.091743  0.994050  0.915966   \n",
      "5 -0.012896      0.857798    -0.012176  0.924181  0.875502   \n",
      "6 -0.012879      0.917431    -0.012895  0.960196  0.920354   \n",
      "7  0.083136      0.917431     0.082569  0.995097  0.923729   \n",
      "8  0.006567      0.839450     0.006173  0.905629  0.860558   \n",
      "\n",
      "   test_imbalanced_f1  test_imbalanced_no_f1  holdout_f1  holdout_no_f1  \n",
      "0            0.568992               0.200000    0.670599       0.380952  \n",
      "1            0.926726               0.857143    0.721699       0.470588  \n",
      "2            0.533272               0.153846    0.618553       0.296296  \n",
      "3            0.569278               0.222222    0.625896       0.307692  \n",
      "4            0.643590               0.333333    0.581530       0.242424  \n",
      "5            0.479010               0.130435    0.485983       0.140351  \n",
      "6            0.551613               0.200000    0.633684       0.320000  \n",
      "7            0.632798               0.315789    0.581530       0.242424  \n",
      "8            0.479010               0.130435    0.476389       0.133333  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Glob√°lis feature_cols defin√≠ci√≥\n",
    "feature_cols = ['AnimalGroup', 'Animal', 'Species', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "\n",
    "# Adatok el≈ëk√©sz√≠t√©se\n",
    "def prepare_data():\n",
    "    # Az el≈ëk√©sz√≠tett adatf√°jl bet√∂lt√©se\n",
    "    data = pd.read_csv('animal_condition_processed_no_encoding.csv')\n",
    "    print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "    data = data.dropna(subset=['Dangerous'])\n",
    "    data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "    valid_values = ['Yes', 'No']\n",
    "    data = data[data['Dangerous'].isin(valid_values)]\n",
    "    print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "    data[feature_cols] = data[feature_cols].fillna('unknown')\n",
    "\n",
    "    # One-Hot Encoding √©s t√≠puskonverzi√≥ numerikusra\n",
    "    X = pd.get_dummies(data[feature_cols], columns=feature_cols).astype(int)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Hold-out set l√©trehoz√°sa (20%)\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # K√ºl√∂n tesztk√©szlet az eredeti adaton (20%)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    \n",
    "    # Oversampling strat√©gi√°k defini√°l√°sa\n",
    "    oversamplers = {\n",
    "        'random_oversampling': RandomOverSampler(sampling_strategy='auto', random_state=42),\n",
    "        'smote': SMOTE(sampling_strategy='auto', random_state=42),\n",
    "        'adasyn': ADASYN(sampling_strategy='auto', random_state=42)\n",
    "    }\n",
    "    \n",
    "    oversampled_data = {}\n",
    "    for name, oversampler in oversamplers.items():\n",
    "        print(f\"\\nApplying {name}...\")\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X_train_val, y_train_val)\n",
    "        resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        resampled_data['Dangerous'] = y_resampled.map({1: 'Yes', 0: 'No'})\n",
    "        resampled_data.to_csv(f'oversampled_{name}.csv', index=False)\n",
    "        print(f\"Class distribution after {name}:\\n\", resampled_data['Dangerous'].value_counts())\n",
    "        oversampled_data[name] = resampled_data\n",
    "    return oversampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, X.columns, data\n",
    "\n",
    "# Adatk√©szletek bet√∂lt√©se vagy l√©trehoz√°sa\n",
    "oversampled_files = {\n",
    "    'random_oversampling': 'oversampled_random_oversampling.csv',\n",
    "    'smote': 'oversampled_smote.csv',\n",
    "    'adasyn': 'oversampled_adasyn.csv'\n",
    "}\n",
    "oversampled_data = {}\n",
    "X_holdout, y_holdout = None, None\n",
    "X_test_imbalanced, y_test_imbalanced = None, None\n",
    "feature_columns = None\n",
    "original_data = None\n",
    "if all(os.path.exists(file) for file in oversampled_files.values()):\n",
    "    print(\"Loading existing oversampled datasets...\")\n",
    "    for name, file in oversampled_files.items():\n",
    "        data = pd.read_csv(file)\n",
    "        original_data = pd.read_csv('animal_condition_processed.csv')\n",
    "        original_data = original_data.dropna(subset=['Dangerous'])\n",
    "        original_data['Dangerous'] = original_data['Dangerous'].str.strip().str.capitalize()\n",
    "        original_data = original_data[original_data['Dangerous'].isin(['Yes', 'No'])]\n",
    "        original_data[feature_cols] = original_data[feature_cols].fillna('unknown')\n",
    "        X_orig = pd.get_dummies(original_data[feature_cols], columns=feature_cols).astype(int)\n",
    "        X = data.drop('Dangerous', axis=1).reindex(columns=X_orig.columns, fill_value=0)\n",
    "        data = X.copy()\n",
    "        data['Dangerous'] = pd.read_csv(file)['Dangerous']\n",
    "        oversampled_data[name] = data\n",
    "        print(f\"Class distribution after loading {name}:\\n\", data['Dangerous'].value_counts())\n",
    "    X_orig_full = X_orig\n",
    "    y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X_orig_full, y_orig_full, test_size=0.2, random_state=42, stratify=y_orig_full)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    feature_columns = X_orig.columns\n",
    "else:\n",
    "    oversampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, feature_columns, original_data = prepare_data()\n",
    "\n",
    "# Keresztvalid√°ci√≥ az eredeti adaton\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_orig_full = pd.get_dummies(original_data[feature_cols], columns=feature_cols).astype(int)\n",
    "y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_scaled = scaler_orig.fit_transform(X_orig_full)\n",
    "\n",
    "# Modellek\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(\n",
    "        class_weight='balanced', random_state=42, n_estimators=100, max_depth=3, min_samples_split=15\n",
    "    ),\n",
    "    'logistic_regression': LogisticRegression(\n",
    "        class_weight='balanced', random_state=42, max_iter=1000, C=0.005\n",
    "    ),\n",
    "    'decision_tree': DecisionTreeClassifier(\n",
    "        class_weight='balanced', random_state=42, max_depth=3, min_samples_split=15\n",
    "    )\n",
    "}\n",
    "\n",
    "# Eredm√©nyek √©s predikci√≥k t√°rol√°sa\n",
    "results = []\n",
    "predictions = []\n",
    "learning_curve_data = []\n",
    "\n",
    "# Keresztvalid√°ci√≥ √©s tesztel√©s\n",
    "for oversampler_name, data in oversampled_data.items():\n",
    "    print(f\"\\nProcessing {oversampler_name} dataset...\")\n",
    "    \n",
    "    X = data.drop('Dangerous', axis=1)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {oversampler_name}...\")\n",
    "        \n",
    "        # Keresztvalid√°ci√≥ a t√∫lmintav√©telezett adaton\n",
    "        cv_f1 = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        # Keresztvalid√°ci√≥ az eredeti imbalanced adaton\n",
    "        cv_f1_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "        \n",
    "        try:\n",
    "            y_test_scores = model.predict_proba(X_test)[:, 1]\n",
    "        except AttributeError:\n",
    "            y_test_scores = model.predict(X_test)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        # Imbalanced tesztk√©szlet teljes√≠tm√©ny\n",
    "        X_test_imbalanced_scaled = scaler.transform(X_test_imbalanced)\n",
    "        y_test_imbalanced_pred = model.predict(X_test_imbalanced_scaled)\n",
    "        test_imbalanced_f1 = f1_score(y_test_imbalanced, y_test_imbalanced_pred, average='macro')\n",
    "        test_imbalanced_roc_auc = roc_auc_score(y_test_imbalanced, y_test_imbalanced_pred)\n",
    "        test_imbalanced_report = classification_report(y_test_imbalanced, y_test_imbalanced_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        \n",
    "        # Hold-out set teljes√≠tm√©ny\n",
    "        X_holdout_scaled = scaler.transform(X_holdout)\n",
    "        y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "        holdout_f1 = f1_score(y_holdout, y_holdout_pred, average='macro')\n",
    "        holdout_roc_auc = roc_auc_score(y_holdout, y_holdout_pred)\n",
    "        holdout_report = classification_report(y_holdout, y_holdout_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        \n",
    "        f1_gap = train_f1 - test_f1\n",
    "        roc_auc_gap = train_roc_auc - test_roc_auc\n",
    "        \n",
    "        report = classification_report(y_test, y_test_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "        \n",
    "        if model_name in ['random_forest', 'decision_tree']:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            print(f\"Feature Importance for {oversampler_name} - {model_name}:\\n\", feature_importance.head(10))\n",
    "            feature_importance.to_csv(f'feature_importance_{oversampler_name}_{model_name}.csv', index=False)\n",
    "        \n",
    "        result = {\n",
    "            'oversampler': oversampler_name,\n",
    "            'model': model_name,\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_std': cv_f1.std(),\n",
    "            'cv_roc_auc_mean': cv_roc_auc.mean(),\n",
    "            'cv_roc_auc_std': cv_roc_auc.std(),\n",
    "            'cv_f1_orig': cv_f1_orig.mean(),\n",
    "            'cv_roc_auc_orig': cv_roc_auc_orig.mean(),\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'f1_gap': f1_gap,\n",
    "            'train_roc_auc': train_roc_auc,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'roc_auc_gap': roc_auc_gap,\n",
    "            'pr_auc': pr_auc,\n",
    "            'no_precision': report['No']['precision'],\n",
    "            'no_recall': report['No']['recall'],\n",
    "            'no_f1': report['No']['f1-score'],\n",
    "            'yes_precision': report['Yes']['precision'],\n",
    "            'yes_recall': report['Yes']['recall'],\n",
    "            'yes_f1': report['Yes']['f1-score'],\n",
    "            'test_imbalanced_f1': test_imbalanced_f1,\n",
    "            'test_imbalanced_roc_auc': test_imbalanced_roc_auc,\n",
    "            'test_imbalanced_no_f1': test_imbalanced_report['No']['f1-score'],\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'holdout_roc_auc': holdout_roc_auc,\n",
    "            'holdout_no_f1': holdout_report['No']['f1-score'],\n",
    "            'confusion_matrix': cm.tolist()\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        pred_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_test_pred,\n",
    "            'proba_yes': y_test_scores\n",
    "        })\n",
    "        pred_df.to_csv(f'predictions_{oversampler_name}_{model_name}.csv', index=False)\n",
    "        \n",
    "        train_sizes, train_scores, valid_scores = learning_curve(\n",
    "            model, X_scaled, y, cv=skf, scoring='f1_macro', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "        )\n",
    "        for size, train_score, valid_score in zip(train_sizes, train_scores.mean(axis=1), valid_scores.mean(axis=1)):\n",
    "            learning_curve_data.append({\n",
    "                'oversampler': oversampler_name,\n",
    "                'model': model_name,\n",
    "                'train_size': size,\n",
    "                'train_f1': train_score,\n",
    "                'valid_f1': valid_score,\n",
    "                'f1_gap': train_score - valid_score\n",
    "            })\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')\n",
    "        plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1')\n",
    "        plt.title(f'Learning Curve: {oversampler_name} - {model_name}')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel('F1 Score (Macro)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'learning_curve_{oversampler_name}_{model_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Eredm√©nyek ment√©se\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "results_df.to_json('classification_results.json', orient='records', lines=True)\n",
    "learning_curve_df = pd.DataFrame(learning_curve_data)\n",
    "learning_curve_df.to_csv('learning_curve_results.csv', index=False)\n",
    "\n",
    "# Eredm√©nyek ki√≠r√°sa\n",
    "print(\"\\nClassification Results with Holdout and Imbalanced Test:\")\n",
    "print(results_df[['oversampler', 'model', 'cv_f1_mean', 'cv_f1_orig', 'test_f1', 'f1_gap', 'test_roc_auc', 'roc_auc_gap', 'pr_auc', 'no_f1', 'test_imbalanced_f1', 'test_imbalanced_no_f1', 'holdout_f1', 'holdout_no_f1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb445af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\n"
     ]
    }
   ],
   "source": [
    "# Teljes kieg√©sz√≠tett k√≥d Luca k√©r√©se alapj√°n\n",
    "# Megjegyz√©s: Az eredeti k√≥d el≈ëtt m√°r szerepelt a k√≥d alapja, itt csak a mappa-struktur√°lt kieg√©sz√≠t√©s l√°that√≥\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# üìÅ Kimeneti mapp√°k l√©trehoz√°sa\n",
    "output_dirs = {\n",
    "    'predictions': 'oversampling_output/predictions',\n",
    "    'feature_importance': 'oversampling_output/feature_importance',\n",
    "    'learning_curves': 'oversampling_output/learning_curves',\n",
    "    'confusion_matrices': 'oversampling_output/confusion_matrices',\n",
    "    'results': 'oversampling_output/results'\n",
    "}\n",
    "\n",
    "for dir_path in output_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# üîÑ M√°r l√©tez≈ë f√°jlok √°thelyez√©se a megfelel≈ë mapp√°kba\n",
    "for res in results:\n",
    "    oversampler = res['oversampler']\n",
    "    model = res['model']\n",
    "\n",
    "    # üîπ Predikci√≥k\n",
    "    pred_file = f'predictions_{oversampler}_{model}.csv'\n",
    "    if os.path.exists(pred_file):\n",
    "        os.rename(pred_file, os.path.join(output_dirs['predictions'], pred_file))\n",
    "\n",
    "    # üîπ Feature importance\n",
    "    importance_file = f'feature_importance_{oversampler}_{model}.csv'\n",
    "    if os.path.exists(importance_file):\n",
    "        os.rename(importance_file, os.path.join(output_dirs['feature_importance'], importance_file))\n",
    "\n",
    "    # üîπ Learning curve\n",
    "    curve_file = f'learning_curve_{oversampler}_{model}.png'\n",
    "    if os.path.exists(curve_file):\n",
    "        os.rename(curve_file, os.path.join(output_dirs['learning_curves'], curve_file))\n",
    "\n",
    "    # üîπ Konf√∫zios m√°trix ment√©se\n",
    "    cm = np.array(res['confusion_matrix'])\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred: No', 'Pred: Yes'],\n",
    "                yticklabels=['Actual: No', 'Actual: Yes'])\n",
    "    plt.title(f'Confusion Matrix\\n{oversampler} + {model}')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.tight_layout()\n",
    "    cm_filename = f'conf_matrix_{oversampler}_{model}.png'\n",
    "    plt.savefig(os.path.join(output_dirs['confusion_matrices'], cm_filename))\n",
    "    plt.close()\n",
    "\n",
    "# üìÑ V√©gs≈ë eredm√©nyek ment√©se mapp√°ba\n",
    "results_df.to_csv(os.path.join(output_dirs['results'], 'classification_results.csv'), index=False)\n",
    "results_df.to_json(os.path.join(output_dirs['results'], 'classification_results.json'), orient='records', lines=True)\n",
    "learning_curve_df.to_csv(os.path.join(output_dirs['results'], 'learning_curve_results.csv'), index=False)\n",
    "\n",
    "print(\"\\nüìÅ Minden f√°jl sikeresen elmentve struktur√°lt mapp√°kba! üòé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a464bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feldolgozott adatok mentve: animal_condition_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adatok bet√∂lt√©se\n",
    "df = pd.read_csv(\"animal_condition.csv\")\n",
    "\n",
    "# 1. Minden sz√∂veges oszlop kisbet≈±sre alak√≠t√°sa\n",
    "text_columns = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5', 'Dangerous']\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].str.strip().str.lower()\n",
    "\n",
    "# 2. √öj oszlopok l√©trehoz√°sa: AnimalGroup, Animal, Species\n",
    "def categorize_animal(animal_name):\n",
    "    animal_name = animal_name.lower().strip()\n",
    "    # Alap√©rtelmezett √©rt√©kek\n",
    "    animal_group = 'unknown'\n",
    "    animal = animal_name\n",
    "    species = animal_name\n",
    "\n",
    "    # Kategoriz√°l√°s\n",
    "    if animal_name in ['dog', 'dogs']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'dog'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['cat']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'cat'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['rabbit']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'rabbit'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['cow', 'cattle', 'buffaloes']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'cattle'\n",
    "        species = animal_name if animal_name != 'cattle' else 'unknown'\n",
    "    elif animal_name in ['horse', 'donkey', 'mules']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'equine'\n",
    "        species = animal_name\n",
    "    elif animal_name in ['deer', 'reindeer', 'elk', 'wapiti', 'mule deer', 'black-tailed deer', 'sika deer', 'white-tailed deer', 'moos']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'deer'\n",
    "        species = animal_name if animal_name != 'deer' else 'unknown'\n",
    "    elif animal_name in ['lion', 'tiger']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'big cat'\n",
    "        species = animal_name\n",
    "    elif animal_name in ['fox', 'wolves', 'hyaenas']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'canid'\n",
    "        species = animal_name\n",
    "    elif animal_name in ['goat', 'goats', 'sheep']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'caprine'\n",
    "        species = animal_name if animal_name != 'goats' else 'goat'\n",
    "    elif animal_name in ['pig', 'pigs']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'pig'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['elephant']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'elephant'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['hamster']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'hamster'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['monkey']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'monkey'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['mammal','mammals']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'unknown'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['chicken', 'fowl', 'duck', 'birds', 'other birds']:\n",
    "        animal_group = 'bird'\n",
    "        animal = 'poultry' if animal_name in ['chicken', 'fowl', 'duck'] else 'other birds'\n",
    "        species = animal_name if animal != 'other birds' else 'unknown'\n",
    "    elif animal_name in ['turtle', 'snake']:\n",
    "        animal_group = 'reptile'\n",
    "        animal = animal_name\n",
    "        species = 'unknown'\n",
    "\n",
    "    return animal_group, animal, species\n",
    "\n",
    "# √öj oszlopok hozz√°ad√°sa\n",
    "df[['AnimalGroup', 'Animal', 'Species']] = df['AnimalName'].apply(lambda x: pd.Series(categorize_animal(x)))\n",
    "\n",
    "# 3. √ñsszes√≠tett adathalmaz\n",
    "df_processed = df[['AnimalGroup', 'Animal', 'Species', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5', 'Dangerous']]\n",
    "\n",
    "# 4. Feldolgozott adatok ment√©se\n",
    "df_processed.to_csv(\"animal_condition_processed.csv\", index=False)\n",
    "print(\"\\nFeldolgozott adatok mentve: animal_condition_processed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
