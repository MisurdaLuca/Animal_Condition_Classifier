{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c83279",
   "metadata": {},
   "source": [
    "## Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e20b707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Dangerous: 2\n",
      "Original class distribution:\n",
      " Dangerous\n",
      "Yes    849\n",
      "No      20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying random_oversampling...\n",
      "Class distribution after random_oversampling:\n",
      " Dangerous\n",
      "Yes    849\n",
      "No     849\n",
      "Name: count, dtype: int64\n",
      "Saved oversampled data to oversampled_random_oversampling.csv, .json, and .parquet\n",
      "\n",
      "Applying smote...\n",
      "Class distribution after smote:\n",
      " Dangerous\n",
      "Yes    849\n",
      "No     849\n",
      "Name: count, dtype: int64\n",
      "Saved oversampled data to oversampled_smote.csv, .json, and .parquet\n",
      "\n",
      "Applying adasyn...\n",
      "Class distribution after adasyn:\n",
      " Dangerous\n",
      "Yes    849\n",
      "No     840\n",
      "Name: count, dtype: int64\n",
      "Saved oversampled data to oversampled_adasyn.csv, .json, and .parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adatok betöltése\n",
    "data = pd.read_csv('animal_condition.csv')\n",
    "\n",
    "# Hiányzó értékek kezelése\n",
    "print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "data = data.dropna(subset=['Dangerous'])\n",
    "data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "valid_values = ['Yes', 'No']\n",
    "data = data[data['Dangerous'].isin(valid_values)]\n",
    "print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "# Jellemzők és célváltozó\n",
    "feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "if data[feature_cols].isnull().sum().any():\n",
    "    data[feature_cols] = data[feature_cols].fillna('Unknown')\n",
    "\n",
    "# Eredeti kategorikus értékek mentése\n",
    "data_original = data.copy()\n",
    "\n",
    "# Kategorikus jellemzők kódolása\n",
    "le_dict = {}\n",
    "for col in feature_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "le_dangerous = LabelEncoder()\n",
    "data['Dangerous'] = le_dangerous.fit_transform(data['Dangerous'].map({'Yes': 1, 'No': 0}))\n",
    "\n",
    "# Jellemzők és célváltozó szétválasztása\n",
    "X = data[feature_cols]\n",
    "y = data['Dangerous']\n",
    "\n",
    "# Túlmintavételezési technikák\n",
    "oversamplers = {\n",
    "    'random_oversampling': RandomOverSampler(random_state=42),\n",
    "    'smote': SMOTE(random_state=42),\n",
    "    'adasyn': ADASYN(random_state=42)\n",
    "}\n",
    "\n",
    "# Eredmények tárolása\n",
    "for name, oversampler in oversamplers.items():\n",
    "    print(f\"\\nApplying {name}...\")\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "    \n",
    "    # Visszaalakítás kategorikus értékekre\n",
    "    resampled_data = pd.DataFrame(X_resampled, columns=feature_cols)\n",
    "    for col in feature_cols:\n",
    "        resampled_data[col] = le_dict[col].inverse_transform(resampled_data[col].astype(int))\n",
    "    resampled_data['Dangerous'] = le_dangerous.inverse_transform(y_resampled)\n",
    "    resampled_data['Dangerous'] = resampled_data['Dangerous'].map({1: 'Yes', 0: 'No'})\n",
    "    \n",
    "    # Osztályeloszlás kiírása\n",
    "    print(f\"Class distribution after {name}:\\n\", resampled_data['Dangerous'].value_counts())\n",
    "    \n",
    "    # Mentés különböző formátumokban\n",
    "    resampled_data.to_csv(f'oversampled_{name}.csv', index=False)\n",
    "    resampled_data.to_json(f'oversampled_{name}.json', orient='records', lines=True)\n",
    "    resampled_data.to_parquet(f'oversampled_{name}.parquet', index=False)\n",
    "    print(f\"Saved oversampled data to oversampled_{name}.csv, .json, and .parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfac0f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing random_oversampling dataset...\n",
      "Training random_forest on random_oversampling...\n",
      "Feature Importance for random_oversampling - random_forest:\n",
      "                                  Feature  Importance\n",
      "546                   symptoms3_Diarrhea    0.052586\n",
      "4                     AnimalName_Chicken    0.042808\n",
      "819          symptoms4_Labored breathing    0.040455\n",
      "472   symptoms2_Yellow or green dropping    0.039445\n",
      "245                   symptoms1_Weakness    0.035639\n",
      "1021         symptoms5_Greenish diarrhea    0.031373\n",
      "651                symptoms3_Slow growth    0.030415\n",
      "911                    symptoms4_Wattles    0.028413\n",
      "916                symptoms4_Weight loss    0.027768\n",
      "306                 symptoms2_Depression    0.027276\n",
      "Training logistic_regression on random_oversampling...\n",
      "Training xgboost on random_oversampling...\n",
      "Feature Importance for random_oversampling - xgboost:\n",
      "                         Feature  Importance\n",
      "191   symptoms1_Severe swellimg    0.345018\n",
      "4            AnimalName_Chicken    0.216001\n",
      "2          AnimalName_Buffaloes    0.065622\n",
      "263  symptoms1_loss of appetite    0.045095\n",
      "349         symptoms2_Hepatitis    0.042076\n",
      "93              symptoms1_Edema    0.039989\n",
      "326         symptoms2_Epistaxis    0.037154\n",
      "620     symptoms3_Paralyzed leg    0.029203\n",
      "245          symptoms1_Weakness    0.028094\n",
      "546          symptoms3_Diarrhea    0.025730\n",
      "\n",
      "Processing smote dataset...\n",
      "Training random_forest on smote...\n",
      "Feature Importance for smote - random_forest:\n",
      "                           Feature  Importance\n",
      "3               AnimalName_Cattle    0.066063\n",
      "4              AnimalName_Chicken    0.063121\n",
      "2            AnimalName_Buffaloes    0.041916\n",
      "109               symptoms1_Fever    0.040923\n",
      "36                 AnimalName_cat    0.033920\n",
      "1021  symptoms5_Greenish diarrhea    0.031169\n",
      "25                 AnimalName_Pig    0.030347\n",
      "12                AnimalName_Fowl    0.029741\n",
      "685            symptoms3_Vomiting    0.028618\n",
      "346     symptoms2_Heavy Breathing    0.026749\n",
      "Training logistic_regression on smote...\n",
      "Training xgboost on smote...\n",
      "Feature Importance for smote - xgboost:\n",
      "                           Feature  Importance\n",
      "4              AnimalName_Chicken    0.073809\n",
      "93                symptoms1_Edema    0.052007\n",
      "3               AnimalName_Cattle    0.050086\n",
      "346     symptoms2_Heavy Breathing    0.046964\n",
      "1021  symptoms5_Greenish diarrhea    0.042220\n",
      "326           symptoms2_Epistaxis    0.041731\n",
      "2            AnimalName_Buffaloes    0.041117\n",
      "109               symptoms1_Fever    0.038006\n",
      "958              symptoms5_Anemia    0.031068\n",
      "306          symptoms2_Depression    0.027890\n",
      "\n",
      "Processing adasyn dataset...\n",
      "Training random_forest on adasyn...\n",
      "Feature Importance for adasyn - random_forest:\n",
      "                     Feature  Importance\n",
      "109         symptoms1_Fever    0.058458\n",
      "3         AnimalName_Cattle    0.049540\n",
      "4        AnimalName_Chicken    0.049247\n",
      "685      symptoms3_Vomiting    0.048187\n",
      "2      AnimalName_Buffaloes    0.047908\n",
      "822      symptoms4_Lethargy    0.034590\n",
      "245      symptoms1_Weakness    0.033688\n",
      "25           AnimalName_Pig    0.031220\n",
      "1122  symptoms5_Weight loss    0.030225\n",
      "1064        symptoms5_Pains    0.029233\n",
      "Training logistic_regression on adasyn...\n",
      "Training xgboost on adasyn...\n",
      "Feature Importance for adasyn - xgboost:\n",
      "                        Feature  Importance\n",
      "4           AnimalName_Chicken    0.073031\n",
      "3            AnimalName_Cattle    0.051885\n",
      "346  symptoms2_Heavy Breathing    0.036472\n",
      "2         AnimalName_Buffaloes    0.036301\n",
      "109            symptoms1_Fever    0.036148\n",
      "158   symptoms1_Neck paralysis    0.031424\n",
      "245         symptoms1_Weakness    0.030467\n",
      "479         symptoms2_diarrhea    0.027929\n",
      "306       symptoms2_Depression    0.025722\n",
      "326        symptoms2_Epistaxis    0.025144\n",
      "\n",
      "Classification Results with Holdout and Imbalanced Test:\n",
      "           oversampler                model  cv_f1_mean  cv_f1_orig   test_f1  \\\n",
      "0  random_oversampling        random_forest    0.984501    0.795834  1.000000   \n",
      "1  random_oversampling  logistic_regression    0.990420    0.932164  0.997059   \n",
      "2  random_oversampling              xgboost    0.999265    0.615044  1.000000   \n",
      "3                smote        random_forest    0.887170    0.795834  0.941174   \n",
      "4                smote  logistic_regression    0.899784    0.932164  0.961756   \n",
      "5                smote              xgboost    0.760162    0.615044  0.782207   \n",
      "6               adasyn        random_forest    0.874118    0.795834  0.908219   \n",
      "7               adasyn  logistic_regression    0.898549    0.932164  0.973361   \n",
      "8               adasyn              xgboost    0.760816    0.615044  0.765960   \n",
      "\n",
      "     f1_gap  test_roc_auc  roc_auc_gap    pr_auc     no_f1  \\\n",
      "0  0.000000      1.000000     0.000000  1.000000  1.000000   \n",
      "1  0.001468      0.997059     0.001468  1.000000  0.997067   \n",
      "2  0.000000      1.000000     0.000000  1.000000  1.000000   \n",
      "3 -0.028068      0.941176    -0.028069  0.982351  0.941520   \n",
      "4  0.035298      0.961765     0.035290  0.996410  0.962319   \n",
      "5  0.011618      0.791176     0.010738  0.990339  0.738007   \n",
      "6 -0.015548      0.908158    -0.015447  0.969296  0.905775   \n",
      "7  0.025158      0.973284     0.025235  0.991369  0.972810   \n",
      "8  0.029602      0.776786     0.026042  0.991673  0.712644   \n",
      "\n",
      "   test_imbalanced_f1  test_imbalanced_no_f1  holdout_f1  holdout_no_f1  \n",
      "0            1.000000               1.000000    1.000000       1.000000  \n",
      "1            0.926726               0.857143    0.942970       0.888889  \n",
      "2            1.000000               1.000000    1.000000       1.000000  \n",
      "3            0.613674               0.285714    0.670599       0.380952  \n",
      "4            0.827757               0.666667    1.000000       1.000000  \n",
      "5            1.000000               1.000000    1.000000       1.000000  \n",
      "6            0.569278               0.222222    0.670599       0.380952  \n",
      "7            0.926726               0.857143    1.000000       1.000000  \n",
      "8            1.000000               1.000000    1.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adatok előkészítése\n",
    "def prepare_data():\n",
    "    data = pd.read_csv('animal_condition.csv')\n",
    "    print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "    data = data.dropna(subset=['Dangerous'])\n",
    "    data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "    valid_values = ['Yes', 'No']\n",
    "    data = data[data['Dangerous'].isin(valid_values)]\n",
    "    print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "    feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "    data[feature_cols] = data[feature_cols].fillna('Unknown')\n",
    "\n",
    "    # One-Hot Encoding\n",
    "    X = pd.get_dummies(data[feature_cols], columns=feature_cols)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Hold-out set létrehozása (20%)\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Külön tesztkészlet az eredeti adaton (20%)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    \n",
    "    oversamplers = {\n",
    "        'random_oversampling': RandomOverSampler(sampling_strategy=200/679, random_state=42),  # 679 = 764 * 0.8\n",
    "        'smote': SMOTE(sampling_strategy=200/679, random_state=42),\n",
    "        'adasyn': ADASYN(sampling_strategy=200/679, random_state=42)\n",
    "    }\n",
    "    \n",
    "    oversampled_data = {}\n",
    "    for name, oversampler in oversamplers.items():\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X_train_val, y_train_val)\n",
    "        resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        resampled_data['Dangerous'] = y_resampled.map({1: 'Yes', 0: 'No'})\n",
    "        oversampled_data[name] = resampled_data\n",
    "        resampled_data.to_csv(f'oversampled_{name}.csv', index=False)\n",
    "        print(f\"Class distribution after {name}:\\n\", resampled_data['Dangerous'].value_counts())\n",
    "    return oversampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, X.columns, data\n",
    "\n",
    "# Adatkészletek betöltése vagy létrehozása\n",
    "oversampled_files = {\n",
    "    'random_oversampling': 'oversampled_random_oversampling.csv',\n",
    "    'smote': 'oversampled_smote.csv',\n",
    "    'adasyn': 'oversampled_adasyn.csv'\n",
    "}\n",
    "oversampled_data = {}\n",
    "X_holdout, y_holdout = None, None\n",
    "X_test_imbalanced, y_test_imbalanced = None, None\n",
    "feature_columns = None\n",
    "original_data = None\n",
    "if all(os.path.exists(file) for file in oversampled_files.values()):\n",
    "    for name, file in oversampled_files.items():\n",
    "        data = pd.read_csv(file)\n",
    "        feature_cols = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "        X = pd.get_dummies(data[feature_cols], columns=feature_cols)\n",
    "        original_data = pd.read_csv('animal_condition.csv')\n",
    "        original_data = original_data.dropna(subset=['Dangerous'])\n",
    "        original_data['Dangerous'] = original_data['Dangerous'].str.strip().str.capitalize()\n",
    "        original_data = original_data[original_data['Dangerous'].isin(['Yes', 'No'])]\n",
    "        original_data[feature_cols] = original_data[feature_cols].fillna('Unknown')\n",
    "        X_orig = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "        X = X.reindex(columns=X_orig.columns, fill_value=0)\n",
    "        data = X.copy()\n",
    "        data['Dangerous'] = pd.read_csv(file)['Dangerous']\n",
    "        oversampled_data[name] = data\n",
    "    X_orig_full = X_orig\n",
    "    y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X_orig_full, y_orig_full, test_size=0.2, random_state=42, stratify=y_orig_full)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    feature_columns = X_orig.columns\n",
    "else:\n",
    "    oversampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, feature_columns, original_data = prepare_data()\n",
    "\n",
    "# Modellek\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(\n",
    "        class_weight='balanced', random_state=42, n_estimators=100, max_depth=3, min_samples_split=15\n",
    "    ),\n",
    "    'logistic_regression': LogisticRegression(\n",
    "        class_weight='balanced', random_state=42, max_iter=1000, C=0.005\n",
    "    ),\n",
    "    'xgboost': XGBClassifier(\n",
    "        scale_pos_weight=849/20, random_state=42, eval_metric='logloss', max_depth=3, reg_lambda=3, alpha=1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Eredmények és predikciók tárolása\n",
    "results = []\n",
    "predictions = []\n",
    "learning_curve_data = []\n",
    "\n",
    "# Keresztvalidáció az eredeti adaton\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_orig_full = pd.get_dummies(original_data[feature_cols], columns=feature_cols)\n",
    "y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_scaled = scaler_orig.fit_transform(X_orig_full)\n",
    "\n",
    "# Keresztvalidáció és tesztelés\n",
    "for oversampler_name, data in oversampled_data.items():\n",
    "    print(f\"\\nProcessing {oversampler_name} dataset...\")\n",
    "    \n",
    "    X = data.drop('Dangerous', axis=1)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {oversampler_name}...\")\n",
    "        \n",
    "        # Keresztvalidáció a túlmintavételezett adaton\n",
    "        cv_f1 = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        # Keresztvalidáció az eredeti imbalanced adaton\n",
    "        cv_f1_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "        \n",
    "        try:\n",
    "            y_test_scores = model.predict_proba(X_test)[:, 1]\n",
    "        except AttributeError:\n",
    "            y_test_scores = model.predict(X_test)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        # Imbalanced tesztkészlet teljesítmény\n",
    "        X_test_imbalanced_scaled = scaler.transform(X_test_imbalanced)\n",
    "        y_test_imbalanced_pred = model.predict(X_test_imbalanced_scaled)\n",
    "        test_imbalanced_f1 = f1_score(y_test_imbalanced, y_test_imbalanced_pred, average='macro')\n",
    "        test_imbalanced_roc_auc = roc_auc_score(y_test_imbalanced, y_test_imbalanced_pred)\n",
    "        test_imbalanced_report = classification_report(y_test_imbalanced, y_test_imbalanced_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        \n",
    "        # Hold-out set teljesítmény\n",
    "        X_holdout_scaled = scaler.transform(X_holdout)\n",
    "        y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "        holdout_f1 = f1_score(y_holdout, y_holdout_pred, average='macro')\n",
    "        holdout_roc_auc = roc_auc_score(y_holdout, y_holdout_pred)\n",
    "        holdout_report = classification_report(y_holdout, y_holdout_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        \n",
    "        f1_gap = train_f1 - test_f1\n",
    "        roc_auc_gap = train_roc_auc - test_roc_auc\n",
    "        \n",
    "        report = classification_report(y_test, y_test_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "        \n",
    "        if model_name in ['random_forest', 'xgboost']:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            print(f\"Feature Importance for {oversampler_name} - {model_name}:\\n\", feature_importance.head(10))\n",
    "            feature_importance.to_csv(f'feature_importance_{oversampler_name}_{model_name}.csv', index=False)\n",
    "        \n",
    "        result = {\n",
    "            'oversampler': oversampler_name,\n",
    "            'model': model_name,\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_std': cv_f1.std(),\n",
    "            'cv_roc_auc_mean': cv_roc_auc.mean(),\n",
    "            'cv_roc_auc_std': cv_roc_auc.std(),\n",
    "            'cv_f1_orig': cv_f1_orig.mean(),\n",
    "            'cv_roc_auc_orig': cv_roc_auc_orig.mean(),\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'f1_gap': f1_gap,\n",
    "            'train_roc_auc': train_roc_auc,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'roc_auc_gap': roc_auc_gap,\n",
    "            'pr_auc': pr_auc,\n",
    "            'no_precision': report['No']['precision'],\n",
    "            'no_recall': report['No']['recall'],\n",
    "            'no_f1': report['No']['f1-score'],\n",
    "            'yes_precision': report['Yes']['precision'],\n",
    "            'yes_recall': report['Yes']['recall'],\n",
    "            'yes_f1': report['Yes']['f1-score'],\n",
    "            'test_imbalanced_f1': test_imbalanced_f1,\n",
    "            'test_imbalanced_roc_auc': test_imbalanced_roc_auc,\n",
    "            'test_imbalanced_no_f1': test_imbalanced_report['No']['f1-score'],\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'holdout_roc_auc': holdout_roc_auc,\n",
    "            'holdout_no_f1': holdout_report['No']['f1-score'],\n",
    "            'confusion_matrix': cm.tolist()\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        pred_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_test_pred,\n",
    "            'proba_yes': y_test_scores\n",
    "        })\n",
    "        pred_df.to_csv(f'predictions_{oversampler_name}_{model_name}.csv', index=False)\n",
    "        \n",
    "        train_sizes, train_scores, valid_scores = learning_curve(\n",
    "            model, X_scaled, y, cv=skf, scoring='f1_macro', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "        )\n",
    "        for size, train_score, valid_score in zip(train_sizes, train_scores.mean(axis=1), valid_scores.mean(axis=1)):\n",
    "            learning_curve_data.append({\n",
    "                'oversampler': oversampler_name,\n",
    "                'model': model_name,\n",
    "                'train_size': size,\n",
    "                'train_f1': train_score,\n",
    "                'valid_f1': valid_score,\n",
    "                'f1_gap': train_score - valid_score\n",
    "            })\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')\n",
    "        plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1')\n",
    "        plt.title(f'Learning Curve: {oversampler_name} - {model_name}')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel('F1 Score (Macro)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'learning_curve_{oversampler_name}_{model_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Eredmények mentése\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "results_df.to_json('classification_results.json', orient='records', lines=True)\n",
    "learning_curve_df = pd.DataFrame(learning_curve_data)\n",
    "learning_curve_df.to_csv('learning_curve_results.csv', index=False)\n",
    "\n",
    "# Eredmények kiírása\n",
    "print(\"\\nClassification Results with Holdout and Imbalanced Test:\")\n",
    "print(results_df[['oversampler', 'model', 'cv_f1_mean', 'cv_f1_orig', 'test_f1', 'f1_gap', 'test_roc_auc', 'roc_auc_gap', 'pr_auc', 'no_f1', 'test_imbalanced_f1', 'test_imbalanced_no_f1', 'holdout_f1', 'holdout_no_f1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10dc18bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing oversampled datasets...\n",
      "Class distribution after loading random_oversampling:\n",
      " Dangerous\n",
      "Yes    543\n",
      "No     543\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading smote:\n",
      " Dangerous\n",
      "Yes    543\n",
      "No     543\n",
      "Name: count, dtype: int64\n",
      "Class distribution after loading adasyn:\n",
      " Dangerous\n",
      "No     546\n",
      "Yes    543\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing random_oversampling dataset...\n",
      "Training random_forest on random_oversampling...\n",
      "Feature Importance for random_oversampling - random_forest:\n",
      "                          Feature  Importance\n",
      "7                  Animal_cattle    0.063824\n",
      "168     symptoms1_neck paralysis    0.062046\n",
      "22             Species_buffaloes    0.058139\n",
      "653   symptoms3_ruffled feathers    0.036683\n",
      "38                 Species_sheep    0.035910\n",
      "262           symptoms1_weakness    0.033116\n",
      "1109          symptoms5_weakness    0.030241\n",
      "93      symptoms1_drooping wings    0.027935\n",
      "666        symptoms3_slow growth    0.026036\n",
      "1014            symptoms5_lesion    0.025991\n",
      "Training logistic_regression on random_oversampling...\n",
      "Training decision_tree on random_oversampling...\n",
      "Feature Importance for random_oversampling - decision_tree:\n",
      "                                 Feature  Importance\n",
      "23                      Species_chicken    0.453446\n",
      "22                    Species_buffaloes    0.235401\n",
      "13                        Animal_monkey    0.234322\n",
      "530                  symptoms3_coughing    0.038382\n",
      "1112              symptoms5_weight loss    0.032631\n",
      "96                      symptoms1_edema    0.005818\n",
      "743                 symptoms4_deability    0.000000\n",
      "747   symptoms4_defienciency in vitamin    0.000000\n",
      "746   symptoms4_decresed egg production    0.000000\n",
      "745             symptoms4_death of lamb    0.000000\n",
      "\n",
      "Processing smote dataset...\n",
      "Training random_forest on smote...\n",
      "Feature Importance for smote - random_forest:\n",
      "                 Feature  Importance\n",
      "7         Animal_cattle    0.086935\n",
      "22    Species_buffaloes    0.077501\n",
      "41      Species_unknown    0.065675\n",
      "38        Species_sheep    0.059004\n",
      "116     symptoms1_fever    0.048731\n",
      "26         Species_duck    0.034810\n",
      "23      Species_chicken    0.030522\n",
      "14   Animal_other birds    0.029845\n",
      "5        Animal_caprine    0.027070\n",
      "1    AnimalGroup_mammal    0.026818\n",
      "Training logistic_regression on smote...\n",
      "Training decision_tree on smote...\n",
      "Feature Importance for smote - decision_tree:\n",
      "                                Feature  Importance\n",
      "41                     Species_unknown    0.417096\n",
      "116                    symptoms1_fever    0.258585\n",
      "5                       Animal_caprine    0.193154\n",
      "351                symptoms2_hepatitis    0.070611\n",
      "13                       Animal_monkey    0.047992\n",
      "530                 symptoms3_coughing    0.012562\n",
      "0                     AnimalGroup_bird    0.000000\n",
      "748              symptoms4_dehydration    0.000000\n",
      "747  symptoms4_defienciency in vitamin    0.000000\n",
      "746  symptoms4_decresed egg production    0.000000\n",
      "\n",
      "Processing adasyn dataset...\n",
      "Training random_forest on adasyn...\n",
      "Feature Importance for adasyn - random_forest:\n",
      "                 Feature  Importance\n",
      "22    Species_buffaloes    0.070421\n",
      "41      Species_unknown    0.060234\n",
      "38        Species_sheep    0.060175\n",
      "5        Animal_caprine    0.053476\n",
      "116     symptoms1_fever    0.052754\n",
      "28         Species_fowl    0.042781\n",
      "7         Animal_cattle    0.041968\n",
      "1049    symptoms5_pains    0.040925\n",
      "26         Species_duck    0.030927\n",
      "15           Animal_pig    0.030576\n",
      "Training logistic_regression on adasyn...\n",
      "Training decision_tree on adasyn...\n",
      "Feature Importance for adasyn - decision_tree:\n",
      "                             Feature  Importance\n",
      "41                  Species_unknown    0.368332\n",
      "116                 symptoms1_fever    0.269988\n",
      "5                    Animal_caprine    0.190648\n",
      "13                    Animal_monkey    0.094019\n",
      "351             symptoms2_hepatitis    0.077012\n",
      "738   symptoms4_consistency of milk    0.000000\n",
      "739  symptoms4_cornea become cloudy    0.000000\n",
      "740              symptoms4_coughing    0.000000\n",
      "741                symptoms4_cramps    0.000000\n",
      "742  symptoms4_crusting of the skin    0.000000\n",
      "\n",
      "Classification Results with Holdout and Imbalanced Test:\n",
      "           oversampler                model  cv_f1_mean  cv_f1_orig   test_f1  \\\n",
      "0  random_oversampling        random_forest    0.934344    0.783296  0.880694   \n",
      "1  random_oversampling  logistic_regression    0.974653    0.932164  0.972456   \n",
      "2  random_oversampling        decision_tree    0.923444    0.565093  0.930865   \n",
      "3                smote        random_forest    0.893467    0.783296  0.848544   \n",
      "4                smote  logistic_regression    0.910630    0.932164  0.907478   \n",
      "5                smote        decision_tree    0.836021    0.565093  0.854863   \n",
      "6               adasyn        random_forest    0.899645    0.783296  0.917320   \n",
      "7               adasyn  logistic_regression    0.891887    0.932164  0.916864   \n",
      "8               adasyn        decision_tree    0.837479    0.565093  0.835684   \n",
      "\n",
      "     f1_gap  test_roc_auc  roc_auc_gap    pr_auc     no_f1  \\\n",
      "0  0.014406      0.880734     0.014427  0.983249  0.878505   \n",
      "1  0.026392      0.972477     0.026371  0.999519  0.973214   \n",
      "2  0.002015      0.931193     0.001987  0.969323  0.935622   \n",
      "3  0.013207      0.848624     0.013127  0.955376  0.845070   \n",
      "4  0.092522      0.908257     0.091743  0.994050  0.915966   \n",
      "5 -0.012896      0.857798    -0.012176  0.924181  0.875502   \n",
      "6 -0.012879      0.917431    -0.012895  0.960196  0.920354   \n",
      "7  0.083136      0.917431     0.082569  0.995097  0.923729   \n",
      "8  0.006567      0.839450     0.006173  0.905629  0.860558   \n",
      "\n",
      "   test_imbalanced_f1  test_imbalanced_no_f1  holdout_f1  holdout_no_f1  \n",
      "0            0.568992               0.200000    0.670599       0.380952  \n",
      "1            0.926726               0.857143    0.721699       0.470588  \n",
      "2            0.533272               0.153846    0.618553       0.296296  \n",
      "3            0.569278               0.222222    0.625896       0.307692  \n",
      "4            0.643590               0.333333    0.581530       0.242424  \n",
      "5            0.479010               0.130435    0.485983       0.140351  \n",
      "6            0.551613               0.200000    0.633684       0.320000  \n",
      "7            0.632798               0.315789    0.581530       0.242424  \n",
      "8            0.479010               0.130435    0.476389       0.133333  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Globális feature_cols definíció\n",
    "feature_cols = ['AnimalGroup', 'Animal', 'Species', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']\n",
    "\n",
    "# Adatok előkészítése\n",
    "def prepare_data():\n",
    "    # Az előkészített adatfájl betöltése\n",
    "    data = pd.read_csv('animal_condition_processed_no_encoding.csv')\n",
    "    print(\"Missing values in Dangerous:\", data['Dangerous'].isnull().sum())\n",
    "    data = data.dropna(subset=['Dangerous'])\n",
    "    data['Dangerous'] = data['Dangerous'].str.strip().str.capitalize()\n",
    "    valid_values = ['Yes', 'No']\n",
    "    data = data[data['Dangerous'].isin(valid_values)]\n",
    "    print(\"Original class distribution:\\n\", data['Dangerous'].value_counts())\n",
    "\n",
    "    data[feature_cols] = data[feature_cols].fillna('unknown')\n",
    "\n",
    "    # One-Hot Encoding és típuskonverzió numerikusra\n",
    "    X = pd.get_dummies(data[feature_cols], columns=feature_cols).astype(int)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Hold-out set létrehozása (20%)\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Külön tesztkészlet az eredeti adaton (20%)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    \n",
    "    # Oversampling stratégiák definiálása\n",
    "    oversamplers = {\n",
    "        'random_oversampling': RandomOverSampler(sampling_strategy='auto', random_state=42),\n",
    "        'smote': SMOTE(sampling_strategy='auto', random_state=42),\n",
    "        'adasyn': ADASYN(sampling_strategy='auto', random_state=42)\n",
    "    }\n",
    "    \n",
    "    oversampled_data = {}\n",
    "    for name, oversampler in oversamplers.items():\n",
    "        print(f\"\\nApplying {name}...\")\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X_train_val, y_train_val)\n",
    "        resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        resampled_data['Dangerous'] = y_resampled.map({1: 'Yes', 0: 'No'})\n",
    "        resampled_data.to_csv(f'oversampled_{name}.csv', index=False)\n",
    "        print(f\"Class distribution after {name}:\\n\", resampled_data['Dangerous'].value_counts())\n",
    "        oversampled_data[name] = resampled_data\n",
    "    return oversampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, X.columns, data\n",
    "\n",
    "# Adatkészletek betöltése vagy létrehozása\n",
    "oversampled_files = {\n",
    "    'random_oversampling': 'oversampled_random_oversampling.csv',\n",
    "    'smote': 'oversampled_smote.csv',\n",
    "    'adasyn': 'oversampled_adasyn.csv'\n",
    "}\n",
    "oversampled_data = {}\n",
    "X_holdout, y_holdout = None, None\n",
    "X_test_imbalanced, y_test_imbalanced = None, None\n",
    "feature_columns = None\n",
    "original_data = None\n",
    "if all(os.path.exists(file) for file in oversampled_files.values()):\n",
    "    print(\"Loading existing oversampled datasets...\")\n",
    "    for name, file in oversampled_files.items():\n",
    "        data = pd.read_csv(file)\n",
    "        original_data = pd.read_csv('animal_condition_processed.csv')\n",
    "        original_data = original_data.dropna(subset=['Dangerous'])\n",
    "        original_data['Dangerous'] = original_data['Dangerous'].str.strip().str.capitalize()\n",
    "        original_data = original_data[original_data['Dangerous'].isin(['Yes', 'No'])]\n",
    "        original_data[feature_cols] = original_data[feature_cols].fillna('unknown')\n",
    "        X_orig = pd.get_dummies(original_data[feature_cols], columns=feature_cols).astype(int)\n",
    "        X = data.drop('Dangerous', axis=1).reindex(columns=X_orig.columns, fill_value=0)\n",
    "        data = X.copy()\n",
    "        data['Dangerous'] = pd.read_csv(file)['Dangerous']\n",
    "        oversampled_data[name] = data\n",
    "        print(f\"Class distribution after loading {name}:\\n\", data['Dangerous'].value_counts())\n",
    "    X_orig_full = X_orig\n",
    "    y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    X_orig, X_holdout, y_orig, y_holdout = train_test_split(X_orig_full, y_orig_full, test_size=0.2, random_state=42, stratify=y_orig_full)\n",
    "    X_train_val, X_test_imbalanced, y_train_val, y_test_imbalanced = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig)\n",
    "    feature_columns = X_orig.columns\n",
    "else:\n",
    "    oversampled_data, X_holdout, y_holdout, X_test_imbalanced, y_test_imbalanced, feature_columns, original_data = prepare_data()\n",
    "\n",
    "# Keresztvalidáció az eredeti adaton\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_orig_full = pd.get_dummies(original_data[feature_cols], columns=feature_cols).astype(int)\n",
    "y_orig_full = original_data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_scaled = scaler_orig.fit_transform(X_orig_full)\n",
    "\n",
    "# Modellek\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(\n",
    "        class_weight='balanced', random_state=42, n_estimators=100, max_depth=3, min_samples_split=15\n",
    "    ),\n",
    "    'logistic_regression': LogisticRegression(\n",
    "        class_weight='balanced', random_state=42, max_iter=1000, C=0.005\n",
    "    ),\n",
    "    'decision_tree': DecisionTreeClassifier(\n",
    "        class_weight='balanced', random_state=42, max_depth=3, min_samples_split=15\n",
    "    )\n",
    "}\n",
    "\n",
    "# Eredmények és predikciók tárolása\n",
    "results = []\n",
    "predictions = []\n",
    "learning_curve_data = []\n",
    "\n",
    "# Keresztvalidáció és tesztelés\n",
    "for oversampler_name, data in oversampled_data.items():\n",
    "    print(f\"\\nProcessing {oversampler_name} dataset...\")\n",
    "    \n",
    "    X = data.drop('Dangerous', axis=1)\n",
    "    y = data['Dangerous'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {oversampler_name}...\")\n",
    "        \n",
    "        # Keresztvalidáció a túlmintavételezett adaton\n",
    "        cv_f1 = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        # Keresztvalidáció az eredeti imbalanced adaton\n",
    "        cv_f1_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='f1_macro')\n",
    "        cv_roc_auc_orig = cross_val_score(model, X_orig_scaled, y_orig_full, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "        \n",
    "        try:\n",
    "            y_test_scores = model.predict_proba(X_test)[:, 1]\n",
    "        except AttributeError:\n",
    "            y_test_scores = model.predict(X_test)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        # Imbalanced tesztkészlet teljesítmény\n",
    "        X_test_imbalanced_scaled = scaler.transform(X_test_imbalanced)\n",
    "        y_test_imbalanced_pred = model.predict(X_test_imbalanced_scaled)\n",
    "        test_imbalanced_f1 = f1_score(y_test_imbalanced, y_test_imbalanced_pred, average='macro')\n",
    "        test_imbalanced_roc_auc = roc_auc_score(y_test_imbalanced, y_test_imbalanced_pred)\n",
    "        test_imbalanced_report = classification_report(y_test_imbalanced, y_test_imbalanced_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        \n",
    "        # Hold-out set teljesítmény\n",
    "        X_holdout_scaled = scaler.transform(X_holdout)\n",
    "        y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "        holdout_f1 = f1_score(y_holdout, y_holdout_pred, average='macro')\n",
    "        holdout_roc_auc = roc_auc_score(y_holdout, y_holdout_pred)\n",
    "        holdout_report = classification_report(y_holdout, y_holdout_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        \n",
    "        f1_gap = train_f1 - test_f1\n",
    "        roc_auc_gap = train_roc_auc - test_roc_auc\n",
    "        \n",
    "        report = classification_report(y_test, y_test_pred, target_names=['No', 'Yes'], output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "        \n",
    "        if model_name in ['random_forest', 'decision_tree']:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            print(f\"Feature Importance for {oversampler_name} - {model_name}:\\n\", feature_importance.head(10))\n",
    "            feature_importance.to_csv(f'feature_importance_{oversampler_name}_{model_name}.csv', index=False)\n",
    "        \n",
    "        result = {\n",
    "            'oversampler': oversampler_name,\n",
    "            'model': model_name,\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_std': cv_f1.std(),\n",
    "            'cv_roc_auc_mean': cv_roc_auc.mean(),\n",
    "            'cv_roc_auc_std': cv_roc_auc.std(),\n",
    "            'cv_f1_orig': cv_f1_orig.mean(),\n",
    "            'cv_roc_auc_orig': cv_roc_auc_orig.mean(),\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'f1_gap': f1_gap,\n",
    "            'train_roc_auc': train_roc_auc,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'roc_auc_gap': roc_auc_gap,\n",
    "            'pr_auc': pr_auc,\n",
    "            'no_precision': report['No']['precision'],\n",
    "            'no_recall': report['No']['recall'],\n",
    "            'no_f1': report['No']['f1-score'],\n",
    "            'yes_precision': report['Yes']['precision'],\n",
    "            'yes_recall': report['Yes']['recall'],\n",
    "            'yes_f1': report['Yes']['f1-score'],\n",
    "            'test_imbalanced_f1': test_imbalanced_f1,\n",
    "            'test_imbalanced_roc_auc': test_imbalanced_roc_auc,\n",
    "            'test_imbalanced_no_f1': test_imbalanced_report['No']['f1-score'],\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'holdout_roc_auc': holdout_roc_auc,\n",
    "            'holdout_no_f1': holdout_report['No']['f1-score'],\n",
    "            'confusion_matrix': cm.tolist()\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        pred_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_test_pred,\n",
    "            'proba_yes': y_test_scores\n",
    "        })\n",
    "        pred_df.to_csv(f'predictions_{oversampler_name}_{model_name}.csv', index=False)\n",
    "        \n",
    "        train_sizes, train_scores, valid_scores = learning_curve(\n",
    "            model, X_scaled, y, cv=skf, scoring='f1_macro', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "        )\n",
    "        for size, train_score, valid_score in zip(train_sizes, train_scores.mean(axis=1), valid_scores.mean(axis=1)):\n",
    "            learning_curve_data.append({\n",
    "                'oversampler': oversampler_name,\n",
    "                'model': model_name,\n",
    "                'train_size': size,\n",
    "                'train_f1': train_score,\n",
    "                'valid_f1': valid_score,\n",
    "                'f1_gap': train_score - valid_score\n",
    "            })\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), label='Train F1')\n",
    "        plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validation F1')\n",
    "        plt.title(f'Learning Curve: {oversampler_name} - {model_name}')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel('F1 Score (Macro)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'learning_curve_{oversampler_name}_{model_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Eredmények mentése\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "results_df.to_json('classification_results.json', orient='records', lines=True)\n",
    "learning_curve_df = pd.DataFrame(learning_curve_data)\n",
    "learning_curve_df.to_csv('learning_curve_results.csv', index=False)\n",
    "\n",
    "# Eredmények kiírása\n",
    "print(\"\\nClassification Results with Holdout and Imbalanced Test:\")\n",
    "print(results_df[['oversampler', 'model', 'cv_f1_mean', 'cv_f1_orig', 'test_f1', 'f1_gap', 'test_roc_auc', 'roc_auc_gap', 'pr_auc', 'no_f1', 'test_imbalanced_f1', 'test_imbalanced_no_f1', 'holdout_f1', 'holdout_no_f1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb445af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Minden fájl sikeresen elmentve strukturált mappákba! 😎\n"
     ]
    }
   ],
   "source": [
    "# Teljes kiegészített kód Luca kérése alapján\n",
    "# Megjegyzés: Az eredeti kód előtt már szerepelt a kód alapja, itt csak a mappa-strukturált kiegészítés látható\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# 📁 Kimeneti mappák létrehozása\n",
    "output_dirs = {\n",
    "    'predictions': 'oversampling_output/predictions',\n",
    "    'feature_importance': 'oversampling_output/feature_importance',\n",
    "    'learning_curves': 'oversampling_output/learning_curves',\n",
    "    'confusion_matrices': 'oversampling_output/confusion_matrices',\n",
    "    'results': 'oversampling_output/results'\n",
    "}\n",
    "\n",
    "for dir_path in output_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# 🔄 Már létező fájlok áthelyezése a megfelelő mappákba\n",
    "for res in results:\n",
    "    oversampler = res['oversampler']\n",
    "    model = res['model']\n",
    "\n",
    "    # 🔹 Predikciók\n",
    "    pred_file = f'predictions_{oversampler}_{model}.csv'\n",
    "    if os.path.exists(pred_file):\n",
    "        os.rename(pred_file, os.path.join(output_dirs['predictions'], pred_file))\n",
    "\n",
    "    # 🔹 Feature importance\n",
    "    importance_file = f'feature_importance_{oversampler}_{model}.csv'\n",
    "    if os.path.exists(importance_file):\n",
    "        os.rename(importance_file, os.path.join(output_dirs['feature_importance'], importance_file))\n",
    "\n",
    "    # 🔹 Learning curve\n",
    "    curve_file = f'learning_curve_{oversampler}_{model}.png'\n",
    "    if os.path.exists(curve_file):\n",
    "        os.rename(curve_file, os.path.join(output_dirs['learning_curves'], curve_file))\n",
    "\n",
    "    # 🔹 Konfúzios mátrix mentése\n",
    "    cm = np.array(res['confusion_matrix'])\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred: No', 'Pred: Yes'],\n",
    "                yticklabels=['Actual: No', 'Actual: Yes'])\n",
    "    plt.title(f'Confusion Matrix\\n{oversampler} + {model}')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.tight_layout()\n",
    "    cm_filename = f'conf_matrix_{oversampler}_{model}.png'\n",
    "    plt.savefig(os.path.join(output_dirs['confusion_matrices'], cm_filename))\n",
    "    plt.close()\n",
    "\n",
    "# 📄 Végső eredmények mentése mappába\n",
    "results_df.to_csv(os.path.join(output_dirs['results'], 'classification_results.csv'), index=False)\n",
    "results_df.to_json(os.path.join(output_dirs['results'], 'classification_results.json'), orient='records', lines=True)\n",
    "learning_curve_df.to_csv(os.path.join(output_dirs['results'], 'learning_curve_results.csv'), index=False)\n",
    "\n",
    "print(\"\\n📁 Minden fájl sikeresen elmentve strukturált mappákba! 😎\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a464bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feldolgozott adatok mentve: animal_condition_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adatok betöltése\n",
    "df = pd.read_csv(\"animal_condition.csv\")\n",
    "\n",
    "# 1. Minden szöveges oszlop kisbetűsre alakítása\n",
    "text_columns = ['AnimalName', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5', 'Dangerous']\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].str.strip().str.lower()\n",
    "\n",
    "# 2. Új oszlopok létrehozása: AnimalGroup, Animal, Species\n",
    "def categorize_animal(animal_name):\n",
    "    animal_name = animal_name.lower().strip()\n",
    "    # Alapértelmezett értékek\n",
    "    animal_group = 'unknown'\n",
    "    animal = animal_name\n",
    "    species = animal_name\n",
    "\n",
    "    # Kategorizálás\n",
    "    if animal_name in ['dog', 'dogs']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'dog'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['cat']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'cat'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['rabbit']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'rabbit'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['cow', 'cattle', 'buffaloes']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'cattle'\n",
    "        species = animal_name if animal_name != 'cattle' else 'unknown'\n",
    "    elif animal_name in ['horse', 'donkey', 'mules']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'equine'\n",
    "        species = animal_name\n",
    "    elif animal_name in ['deer', 'reindeer', 'elk', 'wapiti', 'mule deer', 'black-tailed deer', 'sika deer', 'white-tailed deer', 'moos']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'deer'\n",
    "        species = animal_name if animal_name != 'deer' else 'unknown'\n",
    "    elif animal_name in ['lion', 'tiger']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'big cat'\n",
    "        species = animal_name\n",
    "    elif animal_name in ['fox', 'wolves', 'hyaenas']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'canid'\n",
    "        species = animal_name\n",
    "    elif animal_name in ['goat', 'goats', 'sheep']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'caprine'\n",
    "        species = animal_name if animal_name != 'goats' else 'goat'\n",
    "    elif animal_name in ['pig', 'pigs']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'pig'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['elephant']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'elephant'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['hamster']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'hamster'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['monkey']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'monkey'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['mammal','mammals']:\n",
    "        animal_group = 'mammal'\n",
    "        animal = 'unknown'\n",
    "        species = 'unknown'\n",
    "    elif animal_name in ['chicken', 'fowl', 'duck', 'birds', 'other birds']:\n",
    "        animal_group = 'bird'\n",
    "        animal = 'poultry' if animal_name in ['chicken', 'fowl', 'duck'] else 'other birds'\n",
    "        species = animal_name if animal != 'other birds' else 'unknown'\n",
    "    elif animal_name in ['turtle', 'snake']:\n",
    "        animal_group = 'reptile'\n",
    "        animal = animal_name\n",
    "        species = 'unknown'\n",
    "\n",
    "    return animal_group, animal, species\n",
    "\n",
    "# Új oszlopok hozzáadása\n",
    "df[['AnimalGroup', 'Animal', 'Species']] = df['AnimalName'].apply(lambda x: pd.Series(categorize_animal(x)))\n",
    "\n",
    "# 3. Összesített adathalmaz\n",
    "df_processed = df[['AnimalGroup', 'Animal', 'Species', 'symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5', 'Dangerous']]\n",
    "\n",
    "# 4. Feldolgozott adatok mentése\n",
    "df_processed.to_csv(\"animal_condition_processed.csv\", index=False)\n",
    "print(\"\\nFeldolgozott adatok mentve: animal_condition_processed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
